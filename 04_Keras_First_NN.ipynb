{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando Keras para Construir uma Rede Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse exercício utilizaremos uma rede neural para predizer diabetes usando a base de dados Pima Diabetes. Começaremos os exercícios treinando uma Floresta Aleatória para utilizar como um baseline da performance da rede. Em seguida, utilizaremos o pacote Keras para construir e treinar uma rede neural e comparar a performance. Vamos testar diferentes estruturas de rede e como isso impacta na performance, tempo de treinamento e nível de overfitting.\n",
    "\n",
    "## UCI Pima Diabetes Dataset\n",
    "\n",
    "* UCI ML Repositiory (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)\n",
    "\n",
    "\n",
    "### Attributes: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A base de dados possui 8 atributos e uma variável alvo representando uma classe binária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from __future__ import absolute_import, division, print_function  # Python 2/3 compatibility\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Importando Keras\n",
    "\n",
    "from keras.models  import Sequential, K\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Carregando a base de dados diretamente pela url\n",
    "\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = [\"times_pregnant\", \"glucose_tolerance_test\", \"blood_pressure\", \"skin_thickness\", \"insulin\", \n",
    "         \"bmi\", \"pedigree_function\", \"age\", \"has_diabetes\"]\n",
    "diabetes_df = pd.read_csv(url, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>times_pregnant</th>\n",
       "      <th>glucose_tolerance_test</th>\n",
       "      <th>blood_pressure</th>\n",
       "      <th>skin_thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree_function</th>\n",
       "      <th>age</th>\n",
       "      <th>has_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>90</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>42.1</td>\n",
       "      <td>0.371</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>7</td>\n",
       "      <td>83</td>\n",
       "      <td>78</td>\n",
       "      <td>26</td>\n",
       "      <td>71</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.767</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>6</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.9</td>\n",
       "      <td>0.188</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>6</td>\n",
       "      <td>137</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.2</td>\n",
       "      <td>0.151</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>3</td>\n",
       "      <td>148</td>\n",
       "      <td>66</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.256</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     times_pregnant  glucose_tolerance_test  blood_pressure  skin_thickness  \\\n",
       "580               0                     151              90              46   \n",
       "82                7                      83              78              26   \n",
       "33                6                      92              92               0   \n",
       "401               6                     137              61               0   \n",
       "166               3                     148              66              25   \n",
       "\n",
       "     insulin   bmi  pedigree_function  age  has_diabetes  \n",
       "580        0  42.1              0.371   21             1  \n",
       "82        71  29.3              0.767   36             0  \n",
       "33         0  19.9              0.188   28             0  \n",
       "401        0  24.2              0.151   55             0  \n",
       "166        0  32.5              0.256   22             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uma pequena olhada nos dados -- se aparecerem muitos \"NaN\" você pode estar com problemas de conexão\n",
    "print(diabetes_df.shape)\n",
    "diabetes_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = diabetes_df.iloc[:, :-1].values\n",
    "y = diabetes_df[\"has_diabetes\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos dividir a base entre treino e teste (75%, 25%)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.34895833333333331, 0.65104166666666663)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.mean(1-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3454861111111111, 0.65451388888888884)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_train), np.mean(1-y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.359375, 0.640625)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test), np.mean(1-y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das estatísticas acima, percebemos que 35% dos pacientes da base tem diabetes e 65% não. Isso significa que podemos obter uma acurácia de 65% sem qualquer modelo, apenas predizendo que ninguém tem diabetes. Para validarmos nosso modelo de redes neurais, calcularemos o AUC e a acurácia para verificar se estamos acima de um baseline simples.\n",
    "\n",
    "## Exercício: Gerando um baseline com Árvore Aleatória\n",
    "Para iniciar:\n",
    "1. Treine uma Floresta Aleatória (várias Árvores de Decisão combinadas) com 200 árvores na base de treino.\n",
    "2. Calcule a acurácia e auc das predições."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Treine a floresta aleatória\n",
    "\n",
    "#n_estimators: number of trees in the forest\n",
    "#max_features: size of the random subsets of features to consider when splitting a node\n",
    "#max_depth:    the maximum depth of the tree\n",
    "#random_state: seed\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.766\n",
      "roc-auc is 0.831\n"
     ]
    }
   ],
   "source": [
    "# Faça predições 0/1 e probabilísticas\n",
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAHiCAYAAADSwATnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGMNZkqTBGM6SJA3GcJYkaTCGsyRJg/kNrBZhUEUYddkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2180404b2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vamos plotar a curva ROC-AUC para validar nosso modelo\n",
    "def plot_roc(y_test, y_pred, model_name):\n",
    "    fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(fpr, tpr, 'k-')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n",
    "    ax.grid(True)\n",
    "    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n",
    "           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n",
    "\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_rf[:, 1], 'RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construindo uma Rede Neural com uma camada escondida\n",
    "\n",
    "Utilizaremos o modelo Sequential para construir rapidamente uma rede neural. Nossa primeira rede será uma rede neural de uma única camada. Temos 8 variáveis de entrada, então a input shape deve ser 8. Vamos começar com uma única camada escondida com 12 nós."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Primeiro vamos normalizar os dados\n",
    "## Isso ajuda o algoritmo de otimização a obter estabilidade numérica.\n",
    "## Como as Árvores de Decisão trabalham com a divisão de valores atributo a atributo, isso não afeta sua performance\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos construir o modelo\n",
    "# Input size tem dimensão 8\n",
    "# 1 camada escondida, 12 neurônios, ativação sigmoid\n",
    "# A camada final tem apenas um nó com ativação sigmoid (padrão para classificação binária)\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Dense(12, input_shape=(8,), activation='sigmoid'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  Vamos verificar um resumo da rede\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questão de compreensão:\n",
    "\n",
    "Por que  temos 121 parâmetros? Faz sentido?\n",
    "\n",
    "\n",
    "Vamos ajustar nosso modelo por 200 iterações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\IntelPython3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/200\n",
      "576/576 [==============================] - 2s 4ms/step - loss: 0.6913 - acc: 0.5573 - val_loss: 0.6866 - val_acc: 0.5729\n",
      "Epoch 2/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.6854 - acc: 0.6111 - val_loss: 0.6814 - val_acc: 0.5781\n",
      "Epoch 3/200\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.6801 - acc: 0.6233 - val_loss: 0.6767 - val_acc: 0.6042\n",
      "Epoch 4/200\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.6753 - acc: 0.6389 - val_loss: 0.6725 - val_acc: 0.6042\n",
      "Epoch 5/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.6710 - acc: 0.6458 - val_loss: 0.6686 - val_acc: 0.6250\n",
      "Epoch 6/200\n",
      "576/576 [==============================] - 0s 115us/step - loss: 0.6670 - acc: 0.6493 - val_loss: 0.6651 - val_acc: 0.6406\n",
      "Epoch 7/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.6634 - acc: 0.6476 - val_loss: 0.6620 - val_acc: 0.6562\n",
      "Epoch 8/200\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.6601 - acc: 0.6406 - val_loss: 0.6591 - val_acc: 0.6615\n",
      "Epoch 9/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.6571 - acc: 0.6476 - val_loss: 0.6564 - val_acc: 0.6667\n",
      "Epoch 10/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.6544 - acc: 0.6545 - val_loss: 0.6540 - val_acc: 0.6667\n",
      "Epoch 11/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.6519 - acc: 0.6597 - val_loss: 0.6518 - val_acc: 0.6562\n",
      "Epoch 12/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.6496 - acc: 0.6615 - val_loss: 0.6498 - val_acc: 0.6562\n",
      "Epoch 13/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.6475 - acc: 0.6597 - val_loss: 0.6479 - val_acc: 0.6562\n",
      "Epoch 14/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.6455 - acc: 0.6597 - val_loss: 0.6462 - val_acc: 0.6510\n",
      "Epoch 15/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.6437 - acc: 0.6580 - val_loss: 0.6446 - val_acc: 0.6510\n",
      "Epoch 16/200\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.6420 - acc: 0.6580 - val_loss: 0.6431 - val_acc: 0.6510\n",
      "Epoch 17/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.6405 - acc: 0.6615 - val_loss: 0.6417 - val_acc: 0.6458\n",
      "Epoch 18/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.6390 - acc: 0.6615 - val_loss: 0.6404 - val_acc: 0.6458\n",
      "Epoch 19/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.6376 - acc: 0.6615 - val_loss: 0.6392 - val_acc: 0.6458\n",
      "Epoch 20/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6363 - acc: 0.6615 - val_loss: 0.6381 - val_acc: 0.6458\n",
      "Epoch 21/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.6351 - acc: 0.6597 - val_loss: 0.6370 - val_acc: 0.6458\n",
      "Epoch 22/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.6340 - acc: 0.6580 - val_loss: 0.6360 - val_acc: 0.6458\n",
      "Epoch 23/200\n",
      "576/576 [==============================] - 0s 129us/step - loss: 0.6329 - acc: 0.6562 - val_loss: 0.6351 - val_acc: 0.6458\n",
      "Epoch 24/200\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.6319 - acc: 0.6562 - val_loss: 0.6342 - val_acc: 0.6406\n",
      "Epoch 25/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6309 - acc: 0.6545 - val_loss: 0.6333 - val_acc: 0.6406\n",
      "Epoch 26/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.6300 - acc: 0.6545 - val_loss: 0.6325 - val_acc: 0.6406\n",
      "Epoch 27/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.6291 - acc: 0.6545 - val_loss: 0.6317 - val_acc: 0.6406\n",
      "Epoch 28/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6283 - acc: 0.6545 - val_loss: 0.6309 - val_acc: 0.6406\n",
      "Epoch 29/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.6275 - acc: 0.6545 - val_loss: 0.6302 - val_acc: 0.6406\n",
      "Epoch 30/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.6267 - acc: 0.6545 - val_loss: 0.6295 - val_acc: 0.6406\n",
      "Epoch 31/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6260 - acc: 0.6545 - val_loss: 0.6288 - val_acc: 0.6406\n",
      "Epoch 32/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.6253 - acc: 0.6545 - val_loss: 0.6282 - val_acc: 0.6406\n",
      "Epoch 33/200\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.6246 - acc: 0.6545 - val_loss: 0.6275 - val_acc: 0.6406\n",
      "Epoch 34/200\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.6239 - acc: 0.6545 - val_loss: 0.6269 - val_acc: 0.6406\n",
      "Epoch 35/200\n",
      "576/576 [==============================] - 0s 108us/step - loss: 0.6232 - acc: 0.6545 - val_loss: 0.6263 - val_acc: 0.6406\n",
      "Epoch 36/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.6226 - acc: 0.6545 - val_loss: 0.6257 - val_acc: 0.6406\n",
      "Epoch 37/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6219 - acc: 0.6545 - val_loss: 0.6251 - val_acc: 0.6406\n",
      "Epoch 38/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6213 - acc: 0.6545 - val_loss: 0.6245 - val_acc: 0.6406\n",
      "Epoch 39/200\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.6207 - acc: 0.6545 - val_loss: 0.6240 - val_acc: 0.6406\n",
      "Epoch 40/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.6201 - acc: 0.6545 - val_loss: 0.6234 - val_acc: 0.6406\n",
      "Epoch 41/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.6195 - acc: 0.6545 - val_loss: 0.6229 - val_acc: 0.6406\n",
      "Epoch 42/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.6190 - acc: 0.6545 - val_loss: 0.6224 - val_acc: 0.6406\n",
      "Epoch 43/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6184 - acc: 0.6545 - val_loss: 0.6218 - val_acc: 0.6406\n",
      "Epoch 44/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.6179 - acc: 0.6545 - val_loss: 0.6213 - val_acc: 0.6406\n",
      "Epoch 45/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.6173 - acc: 0.6545 - val_loss: 0.6208 - val_acc: 0.6406\n",
      "Epoch 46/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.6168 - acc: 0.6545 - val_loss: 0.6203 - val_acc: 0.6406\n",
      "Epoch 47/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.6163 - acc: 0.6545 - val_loss: 0.6198 - val_acc: 0.6406\n",
      "Epoch 48/200\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.6157 - acc: 0.6545 - val_loss: 0.6193 - val_acc: 0.6406\n",
      "Epoch 49/200\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.6152 - acc: 0.6545 - val_loss: 0.6188 - val_acc: 0.6406\n",
      "Epoch 50/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.6147 - acc: 0.6545 - val_loss: 0.6183 - val_acc: 0.6406\n",
      "Epoch 51/200\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.6142 - acc: 0.6545 - val_loss: 0.6178 - val_acc: 0.6406\n",
      "Epoch 52/200\n",
      "576/576 [==============================] - 0s 104us/step - loss: 0.6137 - acc: 0.6545 - val_loss: 0.6174 - val_acc: 0.6406\n",
      "Epoch 53/200\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.6132 - acc: 0.6545 - val_loss: 0.6169 - val_acc: 0.6406\n",
      "Epoch 54/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.6127 - acc: 0.6545 - val_loss: 0.6164 - val_acc: 0.6406\n",
      "Epoch 55/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.6122 - acc: 0.6545 - val_loss: 0.6159 - val_acc: 0.6406\n",
      "Epoch 56/200\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.6117 - acc: 0.6545 - val_loss: 0.6155 - val_acc: 0.6406\n",
      "Epoch 57/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.6112 - acc: 0.6545 - val_loss: 0.6150 - val_acc: 0.6406\n",
      "Epoch 58/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.6108 - acc: 0.6545 - val_loss: 0.6145 - val_acc: 0.6406\n",
      "Epoch 59/200\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.6103 - acc: 0.6545 - val_loss: 0.6141 - val_acc: 0.6406\n",
      "Epoch 60/200\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.6098 - acc: 0.6545 - val_loss: 0.6136 - val_acc: 0.6406\n",
      "Epoch 61/200\n",
      "576/576 [==============================] - 0s 44us/step - loss: 0.6094 - acc: 0.6545 - val_loss: 0.6132 - val_acc: 0.6406\n",
      "Epoch 62/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.6088 - acc: 0.6545 - val_loss: 0.6127 - val_acc: 0.6406\n",
      "Epoch 63/200\n",
      "576/576 [==============================] - 0s 102us/step - loss: 0.6084 - acc: 0.6545 - val_loss: 0.6123 - val_acc: 0.6406\n",
      "Epoch 64/200\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.6079 - acc: 0.6545 - val_loss: 0.6118 - val_acc: 0.6406\n",
      "Epoch 65/200\n",
      "576/576 [==============================] - 0s 103us/step - loss: 0.6075 - acc: 0.6545 - val_loss: 0.6114 - val_acc: 0.6406\n",
      "Epoch 66/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.6070 - acc: 0.6545 - val_loss: 0.6109 - val_acc: 0.6406\n",
      "Epoch 67/200\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.6065 - acc: 0.6562 - val_loss: 0.6105 - val_acc: 0.6406\n",
      "Epoch 68/200\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.6061 - acc: 0.6562 - val_loss: 0.6100 - val_acc: 0.6458\n",
      "Epoch 69/200\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.6056 - acc: 0.6562 - val_loss: 0.6096 - val_acc: 0.6458\n",
      "Epoch 70/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.6052 - acc: 0.6562 - val_loss: 0.6091 - val_acc: 0.6458\n",
      "Epoch 71/200\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.6047 - acc: 0.6562 - val_loss: 0.6087 - val_acc: 0.6458\n",
      "Epoch 72/200\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.6043 - acc: 0.6562 - val_loss: 0.6083 - val_acc: 0.6458\n",
      "Epoch 73/200\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.6038 - acc: 0.6545 - val_loss: 0.6078 - val_acc: 0.6458\n",
      "Epoch 74/200\n",
      "576/576 [==============================] - 0s 102us/step - loss: 0.6034 - acc: 0.6545 - val_loss: 0.6074 - val_acc: 0.6458\n",
      "Epoch 75/200\n",
      "576/576 [==============================] - 0s 109us/step - loss: 0.6029 - acc: 0.6562 - val_loss: 0.6069 - val_acc: 0.6458\n",
      "Epoch 76/200\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.6025 - acc: 0.6562 - val_loss: 0.6065 - val_acc: 0.6510\n",
      "Epoch 77/200\n",
      "576/576 [==============================] - 0s 110us/step - loss: 0.6021 - acc: 0.6562 - val_loss: 0.6061 - val_acc: 0.6510\n",
      "Epoch 78/200\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.6016 - acc: 0.6562 - val_loss: 0.6057 - val_acc: 0.6510\n",
      "Epoch 79/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.6012 - acc: 0.6562 - val_loss: 0.6052 - val_acc: 0.6510\n",
      "Epoch 80/200\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.6007 - acc: 0.6562 - val_loss: 0.6048 - val_acc: 0.6510\n",
      "Epoch 81/200\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.6003 - acc: 0.6562 - val_loss: 0.6044 - val_acc: 0.6562\n",
      "Epoch 82/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5998 - acc: 0.6580 - val_loss: 0.6040 - val_acc: 0.6562\n",
      "Epoch 83/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5994 - acc: 0.6580 - val_loss: 0.6035 - val_acc: 0.6562\n",
      "Epoch 84/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5990 - acc: 0.6580 - val_loss: 0.6031 - val_acc: 0.6562\n",
      "Epoch 85/200\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5986 - acc: 0.6580 - val_loss: 0.6027 - val_acc: 0.6615\n",
      "Epoch 86/200\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.5981 - acc: 0.6580 - val_loss: 0.6023 - val_acc: 0.6615\n",
      "Epoch 87/200\n",
      "576/576 [==============================] - 0s 32us/step - loss: 0.5977 - acc: 0.6580 - val_loss: 0.6018 - val_acc: 0.6615\n",
      "Epoch 88/200\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.5973 - acc: 0.6580 - val_loss: 0.6014 - val_acc: 0.6615\n",
      "Epoch 89/200\n",
      "576/576 [==============================] - 0s 27us/step - loss: 0.5968 - acc: 0.6580 - val_loss: 0.6010 - val_acc: 0.6615\n",
      "Epoch 90/200\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5964 - acc: 0.6580 - val_loss: 0.6006 - val_acc: 0.6615\n",
      "Epoch 91/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5960 - acc: 0.6580 - val_loss: 0.6002 - val_acc: 0.6615\n",
      "Epoch 92/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.5956 - acc: 0.6580 - val_loss: 0.5998 - val_acc: 0.6615\n",
      "Epoch 93/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5952 - acc: 0.6580 - val_loss: 0.5994 - val_acc: 0.6615\n",
      "Epoch 94/200\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5948 - acc: 0.6580 - val_loss: 0.5990 - val_acc: 0.6615\n",
      "Epoch 95/200\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5943 - acc: 0.6580 - val_loss: 0.5985 - val_acc: 0.6615\n",
      "Epoch 96/200\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.5939 - acc: 0.6580 - val_loss: 0.5981 - val_acc: 0.6615\n",
      "Epoch 97/200\n",
      "576/576 [==============================] - 0s 112us/step - loss: 0.5935 - acc: 0.6580 - val_loss: 0.5977 - val_acc: 0.6615\n",
      "Epoch 98/200\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5931 - acc: 0.6580 - val_loss: 0.5973 - val_acc: 0.6615\n",
      "Epoch 99/200\n",
      "576/576 [==============================] - 0s 100us/step - loss: 0.5927 - acc: 0.6580 - val_loss: 0.5969 - val_acc: 0.6615\n",
      "Epoch 100/200\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5923 - acc: 0.6580 - val_loss: 0.5965 - val_acc: 0.6615\n",
      "Epoch 101/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5919 - acc: 0.6580 - val_loss: 0.5961 - val_acc: 0.6615\n",
      "Epoch 102/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5914 - acc: 0.6580 - val_loss: 0.5957 - val_acc: 0.6615\n",
      "Epoch 103/200\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.5910 - acc: 0.6580 - val_loss: 0.5953 - val_acc: 0.6615\n",
      "Epoch 104/200\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.6542 - acc: 0.625 - 0s 81us/step - loss: 0.5906 - acc: 0.6580 - val_loss: 0.5949 - val_acc: 0.6667\n",
      "Epoch 105/200\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.5902 - acc: 0.6580 - val_loss: 0.5945 - val_acc: 0.6667\n",
      "Epoch 106/200\n",
      "576/576 [==============================] - 0s 103us/step - loss: 0.5898 - acc: 0.6580 - val_loss: 0.5941 - val_acc: 0.6667\n",
      "Epoch 107/200\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5894 - acc: 0.6580 - val_loss: 0.5937 - val_acc: 0.6667\n",
      "Epoch 108/200\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.5890 - acc: 0.6615 - val_loss: 0.5933 - val_acc: 0.6667\n",
      "Epoch 109/200\n",
      "576/576 [==============================] - 0s 102us/step - loss: 0.5886 - acc: 0.6615 - val_loss: 0.5929 - val_acc: 0.6667\n",
      "Epoch 110/200\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.5882 - acc: 0.6615 - val_loss: 0.5926 - val_acc: 0.6667\n",
      "Epoch 111/200\n",
      "576/576 [==============================] - 0s 46us/step - loss: 0.5878 - acc: 0.6615 - val_loss: 0.5922 - val_acc: 0.6667\n",
      "Epoch 112/200\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4977 - acc: 0.781 - 0s 87us/step - loss: 0.5874 - acc: 0.6615 - val_loss: 0.5918 - val_acc: 0.6667\n",
      "Epoch 113/200\n",
      "576/576 [==============================] - 0s 104us/step - loss: 0.5870 - acc: 0.6632 - val_loss: 0.5914 - val_acc: 0.6667\n",
      "Epoch 114/200\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5866 - acc: 0.6632 - val_loss: 0.5910 - val_acc: 0.6667\n",
      "Epoch 115/200\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5862 - acc: 0.6632 - val_loss: 0.5906 - val_acc: 0.6667\n",
      "Epoch 116/200\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5858 - acc: 0.6632 - val_loss: 0.5902 - val_acc: 0.6667\n",
      "Epoch 117/200\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.5854 - acc: 0.6632 - val_loss: 0.5898 - val_acc: 0.6667\n",
      "Epoch 118/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5850 - acc: 0.6632 - val_loss: 0.5895 - val_acc: 0.6667\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 71us/step - loss: 0.5846 - acc: 0.6649 - val_loss: 0.5891 - val_acc: 0.6667\n",
      "Epoch 120/200\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.6695 - acc: 0.531 - 0s 76us/step - loss: 0.5842 - acc: 0.6649 - val_loss: 0.5887 - val_acc: 0.6667\n",
      "Epoch 121/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5838 - acc: 0.6649 - val_loss: 0.5883 - val_acc: 0.6667\n",
      "Epoch 122/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5834 - acc: 0.6649 - val_loss: 0.5879 - val_acc: 0.6667\n",
      "Epoch 123/200\n",
      "576/576 [==============================] - 0s 130us/step - loss: 0.5831 - acc: 0.6667 - val_loss: 0.5876 - val_acc: 0.6667\n",
      "Epoch 124/200\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.5826 - acc: 0.6684 - val_loss: 0.5872 - val_acc: 0.6667\n",
      "Epoch 125/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5823 - acc: 0.6684 - val_loss: 0.5868 - val_acc: 0.6719\n",
      "Epoch 126/200\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5819 - acc: 0.6719 - val_loss: 0.5864 - val_acc: 0.6719\n",
      "Epoch 127/200\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5815 - acc: 0.6719 - val_loss: 0.5861 - val_acc: 0.6719\n",
      "Epoch 128/200\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5811 - acc: 0.6719 - val_loss: 0.5857 - val_acc: 0.6719\n",
      "Epoch 129/200\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5807 - acc: 0.6736 - val_loss: 0.5853 - val_acc: 0.6719\n",
      "Epoch 130/200\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5804 - acc: 0.6736 - val_loss: 0.5849 - val_acc: 0.6719\n",
      "Epoch 131/200\n",
      "576/576 [==============================] - 0s 101us/step - loss: 0.5800 - acc: 0.6736 - val_loss: 0.5846 - val_acc: 0.6719\n",
      "Epoch 132/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5796 - acc: 0.6736 - val_loss: 0.5842 - val_acc: 0.6719\n",
      "Epoch 133/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5792 - acc: 0.6736 - val_loss: 0.5838 - val_acc: 0.6719\n",
      "Epoch 134/200\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5788 - acc: 0.6719 - val_loss: 0.5835 - val_acc: 0.6719\n",
      "Epoch 135/200\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.5785 - acc: 0.6719 - val_loss: 0.5831 - val_acc: 0.6719\n",
      "Epoch 136/200\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.5780 - acc: 0.6736 - val_loss: 0.5827 - val_acc: 0.6771\n",
      "Epoch 137/200\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.5777 - acc: 0.6736 - val_loss: 0.5824 - val_acc: 0.6771\n",
      "Epoch 138/200\n",
      "576/576 [==============================] - 0s 101us/step - loss: 0.5773 - acc: 0.6753 - val_loss: 0.5820 - val_acc: 0.6771\n",
      "Epoch 139/200\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.5770 - acc: 0.6753 - val_loss: 0.5816 - val_acc: 0.6771\n",
      "Epoch 140/200\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5766 - acc: 0.6753 - val_loss: 0.5813 - val_acc: 0.6771\n",
      "Epoch 141/200\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5762 - acc: 0.6753 - val_loss: 0.5809 - val_acc: 0.6771\n",
      "Epoch 142/200\n",
      "576/576 [==============================] - 0s 100us/step - loss: 0.5758 - acc: 0.6788 - val_loss: 0.5806 - val_acc: 0.6823\n",
      "Epoch 143/200\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5755 - acc: 0.6788 - val_loss: 0.5802 - val_acc: 0.6823\n",
      "Epoch 144/200\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5751 - acc: 0.6823 - val_loss: 0.5798 - val_acc: 0.6823\n",
      "Epoch 145/200\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.5747 - acc: 0.6823 - val_loss: 0.5795 - val_acc: 0.6823\n",
      "Epoch 146/200\n",
      "576/576 [==============================] - 0s 100us/step - loss: 0.5743 - acc: 0.6823 - val_loss: 0.5791 - val_acc: 0.6823\n",
      "Epoch 147/200\n",
      "576/576 [==============================] - 0s 102us/step - loss: 0.5740 - acc: 0.6823 - val_loss: 0.5788 - val_acc: 0.6823\n",
      "Epoch 148/200\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5736 - acc: 0.6840 - val_loss: 0.5784 - val_acc: 0.6823\n",
      "Epoch 149/200\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.5733 - acc: 0.6840 - val_loss: 0.5781 - val_acc: 0.6823\n",
      "Epoch 150/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5729 - acc: 0.6840 - val_loss: 0.5777 - val_acc: 0.6875\n",
      "Epoch 151/200\n",
      "576/576 [==============================] - 0s 105us/step - loss: 0.5725 - acc: 0.6840 - val_loss: 0.5774 - val_acc: 0.6875\n",
      "Epoch 152/200\n",
      "576/576 [==============================] - 0s 101us/step - loss: 0.5721 - acc: 0.6875 - val_loss: 0.5770 - val_acc: 0.6875\n",
      "Epoch 153/200\n",
      "576/576 [==============================] - 0s 103us/step - loss: 0.5718 - acc: 0.6858 - val_loss: 0.5767 - val_acc: 0.6875\n",
      "Epoch 154/200\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5714 - acc: 0.6875 - val_loss: 0.5763 - val_acc: 0.6875\n",
      "Epoch 155/200\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.5711 - acc: 0.6875 - val_loss: 0.5760 - val_acc: 0.6927\n",
      "Epoch 156/200\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5707 - acc: 0.6875 - val_loss: 0.5756 - val_acc: 0.6979\n",
      "Epoch 157/200\n",
      "576/576 [==============================] - 0s 131us/step - loss: 0.5703 - acc: 0.6875 - val_loss: 0.5753 - val_acc: 0.6979\n",
      "Epoch 158/200\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.5700 - acc: 0.6858 - val_loss: 0.5750 - val_acc: 0.6979\n",
      "Epoch 159/200\n",
      "576/576 [==============================] - 0s 101us/step - loss: 0.5697 - acc: 0.6858 - val_loss: 0.5746 - val_acc: 0.6979\n",
      "Epoch 160/200\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5693 - acc: 0.6858 - val_loss: 0.5743 - val_acc: 0.6979\n",
      "Epoch 161/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5690 - acc: 0.6858 - val_loss: 0.5739 - val_acc: 0.6979\n",
      "Epoch 162/200\n",
      "576/576 [==============================] - 0s 159us/step - loss: 0.5686 - acc: 0.6858 - val_loss: 0.5736 - val_acc: 0.6979\n",
      "Epoch 163/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5682 - acc: 0.6840 - val_loss: 0.5733 - val_acc: 0.6979\n",
      "Epoch 164/200\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5679 - acc: 0.6840 - val_loss: 0.5729 - val_acc: 0.7031\n",
      "Epoch 165/200\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5675 - acc: 0.6858 - val_loss: 0.5726 - val_acc: 0.7031\n",
      "Epoch 166/200\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.5672 - acc: 0.6858 - val_loss: 0.5722 - val_acc: 0.7031\n",
      "Epoch 167/200\n",
      "576/576 [==============================] - 0s 101us/step - loss: 0.5668 - acc: 0.6858 - val_loss: 0.5719 - val_acc: 0.7031\n",
      "Epoch 168/200\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5665 - acc: 0.6875 - val_loss: 0.5716 - val_acc: 0.7031\n",
      "Epoch 169/200\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5661 - acc: 0.6875 - val_loss: 0.5712 - val_acc: 0.7031\n",
      "Epoch 170/200\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5658 - acc: 0.6910 - val_loss: 0.5709 - val_acc: 0.7031\n",
      "Epoch 171/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5655 - acc: 0.6910 - val_loss: 0.5706 - val_acc: 0.7031\n",
      "Epoch 172/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5651 - acc: 0.6892 - val_loss: 0.5702 - val_acc: 0.7031\n",
      "Epoch 173/200\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5648 - acc: 0.6892 - val_loss: 0.5699 - val_acc: 0.7031\n",
      "Epoch 174/200\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5644 - acc: 0.6875 - val_loss: 0.5696 - val_acc: 0.7083\n",
      "Epoch 175/200\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.5641 - acc: 0.6892 - val_loss: 0.5693 - val_acc: 0.7083\n",
      "Epoch 176/200\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5638 - acc: 0.6892 - val_loss: 0.5689 - val_acc: 0.7083\n",
      "Epoch 177/200\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.6191 - acc: 0.625 - 0s 57us/step - loss: 0.5634 - acc: 0.6910 - val_loss: 0.5686 - val_acc: 0.7083\n",
      "Epoch 178/200\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5630 - acc: 0.6910 - val_loss: 0.5683 - val_acc: 0.7083\n",
      "Epoch 179/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5627 - acc: 0.6910 - val_loss: 0.5680 - val_acc: 0.7083\n",
      "Epoch 180/200\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.5624 - acc: 0.6910 - val_loss: 0.5676 - val_acc: 0.7135\n",
      "Epoch 181/200\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5620 - acc: 0.6910 - val_loss: 0.5673 - val_acc: 0.7135\n",
      "Epoch 182/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5617 - acc: 0.6910 - val_loss: 0.5670 - val_acc: 0.7135\n",
      "Epoch 183/200\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5614 - acc: 0.6910 - val_loss: 0.5667 - val_acc: 0.7135\n",
      "Epoch 184/200\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5610 - acc: 0.6892 - val_loss: 0.5664 - val_acc: 0.7135\n",
      "Epoch 185/200\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.5607 - acc: 0.6892 - val_loss: 0.5660 - val_acc: 0.7135\n",
      "Epoch 186/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5604 - acc: 0.6892 - val_loss: 0.5657 - val_acc: 0.7135\n",
      "Epoch 187/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5600 - acc: 0.6892 - val_loss: 0.5654 - val_acc: 0.7135\n",
      "Epoch 188/200\n",
      "576/576 [==============================] - 0s 132us/step - loss: 0.5597 - acc: 0.6892 - val_loss: 0.5651 - val_acc: 0.7135\n",
      "Epoch 189/200\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5594 - acc: 0.6892 - val_loss: 0.5648 - val_acc: 0.7188\n",
      "Epoch 190/200\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5591 - acc: 0.6892 - val_loss: 0.5645 - val_acc: 0.7188\n",
      "Epoch 191/200\n",
      "576/576 [==============================] - 0s 65us/step - loss: 0.5587 - acc: 0.6944 - val_loss: 0.5642 - val_acc: 0.7188\n",
      "Epoch 192/200\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5584 - acc: 0.6927 - val_loss: 0.5638 - val_acc: 0.7188\n",
      "Epoch 193/200\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5580 - acc: 0.6944 - val_loss: 0.5635 - val_acc: 0.7188\n",
      "Epoch 194/200\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5577 - acc: 0.6944 - val_loss: 0.5632 - val_acc: 0.7240\n",
      "Epoch 195/200\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5574 - acc: 0.6962 - val_loss: 0.5629 - val_acc: 0.7240\n",
      "Epoch 196/200\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.5571 - acc: 0.6962 - val_loss: 0.5626 - val_acc: 0.7240\n",
      "Epoch 197/200\n",
      "576/576 [==============================] - 0s 100us/step - loss: 0.5568 - acc: 0.6979 - val_loss: 0.5623 - val_acc: 0.7240\n",
      "Epoch 198/200\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5564 - acc: 0.6997 - val_loss: 0.5620 - val_acc: 0.7240\n",
      "Epoch 199/200\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.5561 - acc: 0.7014 - val_loss: 0.5617 - val_acc: 0.7188\n",
      "Epoch 200/200\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5558 - acc: 0.7031 - val_loss: 0.5614 - val_acc: 0.7188\n"
     ]
    }
   ],
   "source": [
    "# Vamos compilar com um otimizador, função de erro e métrica de avaliação\n",
    "# Roc-Auc não está disponível no Keras, vamos pular seu uso por ora\n",
    "\n",
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n",
    "# o método fit retorna um histórico que pode ser útil para análise dos parâmetros do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 0s 26us/step\n"
     ]
    }
   ],
   "source": [
    "## Da mesma forma que fizemos com o Random Forest, vamos gerar as predições\n",
    "#  com predict_classes (0/1) e predict (probabilidade)\n",
    "\n",
    "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos verificar as 10 primeiras predições\n",
    "y_pred_class_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36724502],\n",
       "       [ 0.56340599],\n",
       "       [ 0.32496291],\n",
       "       [ 0.38223127],\n",
       "       [ 0.23619023],\n",
       "       [ 0.42656457],\n",
       "       [ 0.20548928],\n",
       "       [ 0.39709237],\n",
       "       [ 0.53303063],\n",
       "       [ 0.3528029 ]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.719\n",
      "roc-auc is 0.797\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAHiCAYAAADSwATnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21803e77dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vamos imprimir a acurácia, roc-auc e plotar a curva ROC-AUC\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_1, 'NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podem haver variações nos números exatos por conta da aleatoriedade, mas você deve obter resultados similares ao Random Forest, entre 75% e 85% de acurácia e entre .8 e .9 para auc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver o que o objeto  `run_hist_1`  contém."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_hist_1.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o histórico para plotar o erro de treino ao longo das gerações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x218057c6e80>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMI+EWTis1p8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21805341748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reparem que os erros parecem estar caindo tanto para o conjunto de treino como de validação. Isso sugere que o modelo ainda poderia melhorar com mais algumas gerações.\n",
    "\n",
    "Vamos treinar o modelo um pouco mais, note que ele não reinicia do zero, mas começará de onde parou. Treine por mais 1000 iterações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.5555 - acc: 0.7066 - val_loss: 0.5611 - val_acc: 0.7188\n",
      "Epoch 2/1000\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.5552 - acc: 0.7066 - val_loss: 0.5608 - val_acc: 0.7240\n",
      "Epoch 3/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5548 - acc: 0.7066 - val_loss: 0.5605 - val_acc: 0.7292\n",
      "Epoch 4/1000\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.5545 - acc: 0.7066 - val_loss: 0.5602 - val_acc: 0.7292\n",
      "Epoch 5/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5542 - acc: 0.7066 - val_loss: 0.5599 - val_acc: 0.7292\n",
      "Epoch 6/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5539 - acc: 0.7083 - val_loss: 0.5596 - val_acc: 0.7292\n",
      "Epoch 7/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5536 - acc: 0.7083 - val_loss: 0.5593 - val_acc: 0.7292\n",
      "Epoch 8/1000\n",
      "576/576 [==============================] - 0s 159us/step - loss: 0.5533 - acc: 0.7118 - val_loss: 0.5590 - val_acc: 0.7240\n",
      "Epoch 9/1000\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.5530 - acc: 0.7135 - val_loss: 0.5587 - val_acc: 0.7240\n",
      "Epoch 10/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5526 - acc: 0.7153 - val_loss: 0.5584 - val_acc: 0.7240\n",
      "Epoch 11/1000\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5523 - acc: 0.7153 - val_loss: 0.5581 - val_acc: 0.7240\n",
      "Epoch 12/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5520 - acc: 0.7170 - val_loss: 0.5578 - val_acc: 0.7240\n",
      "Epoch 13/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5517 - acc: 0.7170 - val_loss: 0.5575 - val_acc: 0.7240\n",
      "Epoch 14/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5514 - acc: 0.7188 - val_loss: 0.5572 - val_acc: 0.7240\n",
      "Epoch 15/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5511 - acc: 0.7188 - val_loss: 0.5569 - val_acc: 0.7240\n",
      "Epoch 16/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5508 - acc: 0.7222 - val_loss: 0.5566 - val_acc: 0.7240\n",
      "Epoch 17/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5505 - acc: 0.7222 - val_loss: 0.5564 - val_acc: 0.7240\n",
      "Epoch 18/1000\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5502 - acc: 0.7222 - val_loss: 0.5561 - val_acc: 0.7240\n",
      "Epoch 19/1000\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.5499 - acc: 0.7222 - val_loss: 0.5558 - val_acc: 0.7240\n",
      "Epoch 20/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5496 - acc: 0.7222 - val_loss: 0.5555 - val_acc: 0.7292\n",
      "Epoch 21/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5492 - acc: 0.7222 - val_loss: 0.5552 - val_acc: 0.7292\n",
      "Epoch 22/1000\n",
      "576/576 [==============================] - 0s 115us/step - loss: 0.5490 - acc: 0.7240 - val_loss: 0.5549 - val_acc: 0.7292\n",
      "Epoch 23/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5487 - acc: 0.7240 - val_loss: 0.5546 - val_acc: 0.7292\n",
      "Epoch 24/1000\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5483 - acc: 0.7240 - val_loss: 0.5544 - val_acc: 0.7292\n",
      "Epoch 25/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5480 - acc: 0.7240 - val_loss: 0.5541 - val_acc: 0.7292\n",
      "Epoch 26/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5477 - acc: 0.7240 - val_loss: 0.5538 - val_acc: 0.7292\n",
      "Epoch 27/1000\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.5474 - acc: 0.7274 - val_loss: 0.5535 - val_acc: 0.7292\n",
      "Epoch 28/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5472 - acc: 0.7274 - val_loss: 0.5532 - val_acc: 0.7292\n",
      "Epoch 29/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5468 - acc: 0.7292 - val_loss: 0.5530 - val_acc: 0.7292\n",
      "Epoch 30/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5466 - acc: 0.7309 - val_loss: 0.5527 - val_acc: 0.7292\n",
      "Epoch 31/1000\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.5463 - acc: 0.7326 - val_loss: 0.5524 - val_acc: 0.7292\n",
      "Epoch 32/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5460 - acc: 0.7326 - val_loss: 0.5521 - val_acc: 0.7292\n",
      "Epoch 33/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5457 - acc: 0.7326 - val_loss: 0.5519 - val_acc: 0.7292\n",
      "Epoch 34/1000\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5454 - acc: 0.7309 - val_loss: 0.5516 - val_acc: 0.7292\n",
      "Epoch 35/1000\n",
      "576/576 [==============================] - 0s 107us/step - loss: 0.5451 - acc: 0.7326 - val_loss: 0.5513 - val_acc: 0.7292\n",
      "Epoch 36/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5448 - acc: 0.7326 - val_loss: 0.5510 - val_acc: 0.7292\n",
      "Epoch 37/1000\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.5445 - acc: 0.7326 - val_loss: 0.5508 - val_acc: 0.7292\n",
      "Epoch 38/1000\n",
      "576/576 [==============================] - 0s 125us/step - loss: 0.5442 - acc: 0.7326 - val_loss: 0.5505 - val_acc: 0.7292\n",
      "Epoch 39/1000\n",
      "576/576 [==============================] - 0s 103us/step - loss: 0.5439 - acc: 0.7344 - val_loss: 0.5502 - val_acc: 0.7344\n",
      "Epoch 40/1000\n",
      "576/576 [==============================] - 0s 113us/step - loss: 0.5436 - acc: 0.7326 - val_loss: 0.5500 - val_acc: 0.7344\n",
      "Epoch 41/1000\n",
      "576/576 [==============================] - 0s 114us/step - loss: 0.5433 - acc: 0.7344 - val_loss: 0.5497 - val_acc: 0.7344\n",
      "Epoch 42/1000\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5431 - acc: 0.7326 - val_loss: 0.5494 - val_acc: 0.7344\n",
      "Epoch 43/1000\n",
      "576/576 [==============================] - 0s 60us/step - loss: 0.5428 - acc: 0.7326 - val_loss: 0.5492 - val_acc: 0.7344\n",
      "Epoch 44/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5425 - acc: 0.7292 - val_loss: 0.5489 - val_acc: 0.7292\n",
      "Epoch 45/1000\n",
      "576/576 [==============================] - 0s 129us/step - loss: 0.5422 - acc: 0.7292 - val_loss: 0.5486 - val_acc: 0.7292\n",
      "Epoch 46/1000\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.5419 - acc: 0.7292 - val_loss: 0.5484 - val_acc: 0.7292\n",
      "Epoch 47/1000\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.5417 - acc: 0.7274 - val_loss: 0.5481 - val_acc: 0.7292\n",
      "Epoch 48/1000\n",
      "576/576 [==============================] - 0s 53us/step - loss: 0.5414 - acc: 0.7274 - val_loss: 0.5478 - val_acc: 0.7292\n",
      "Epoch 49/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5411 - acc: 0.7274 - val_loss: 0.5476 - val_acc: 0.7344\n",
      "Epoch 50/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5408 - acc: 0.7274 - val_loss: 0.5473 - val_acc: 0.7344\n",
      "Epoch 51/1000\n",
      "576/576 [==============================] - 0s 58us/step - loss: 0.5405 - acc: 0.7274 - val_loss: 0.5471 - val_acc: 0.7344\n",
      "Epoch 52/1000\n",
      "576/576 [==============================] - 0s 72us/step - loss: 0.5402 - acc: 0.7274 - val_loss: 0.5468 - val_acc: 0.7344\n",
      "Epoch 53/1000\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5400 - acc: 0.7274 - val_loss: 0.5466 - val_acc: 0.7344\n",
      "Epoch 54/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5397 - acc: 0.7274 - val_loss: 0.5463 - val_acc: 0.7396\n",
      "Epoch 55/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5394 - acc: 0.7292 - val_loss: 0.5460 - val_acc: 0.7344\n",
      "Epoch 56/1000\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.5391 - acc: 0.7309 - val_loss: 0.5458 - val_acc: 0.7344\n",
      "Epoch 57/1000\n",
      "576/576 [==============================] - 0s 108us/step - loss: 0.5389 - acc: 0.7309 - val_loss: 0.5455 - val_acc: 0.7344\n",
      "Epoch 58/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5386 - acc: 0.7326 - val_loss: 0.5453 - val_acc: 0.7344\n",
      "Epoch 59/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5383 - acc: 0.7326 - val_loss: 0.5450 - val_acc: 0.7344\n",
      "Epoch 60/1000\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5381 - acc: 0.7309 - val_loss: 0.5448 - val_acc: 0.7344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/1000\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.5378 - acc: 0.7309 - val_loss: 0.5445 - val_acc: 0.7344\n",
      "Epoch 62/1000\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5375 - acc: 0.7326 - val_loss: 0.5443 - val_acc: 0.7344\n",
      "Epoch 63/1000\n",
      "576/576 [==============================] - 0s 109us/step - loss: 0.5372 - acc: 0.7326 - val_loss: 0.5440 - val_acc: 0.7344\n",
      "Epoch 64/1000\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.5370 - acc: 0.7344 - val_loss: 0.5438 - val_acc: 0.7344\n",
      "Epoch 65/1000\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5367 - acc: 0.7344 - val_loss: 0.5435 - val_acc: 0.7344\n",
      "Epoch 66/1000\n",
      "576/576 [==============================] - 0s 123us/step - loss: 0.5364 - acc: 0.7344 - val_loss: 0.5433 - val_acc: 0.7344\n",
      "Epoch 67/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5362 - acc: 0.7361 - val_loss: 0.5431 - val_acc: 0.7344\n",
      "Epoch 68/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5359 - acc: 0.7361 - val_loss: 0.5428 - val_acc: 0.7344\n",
      "Epoch 69/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5357 - acc: 0.7361 - val_loss: 0.5426 - val_acc: 0.7344\n",
      "Epoch 70/1000\n",
      "576/576 [==============================] - 0s 139us/step - loss: 0.5354 - acc: 0.7361 - val_loss: 0.5423 - val_acc: 0.7344\n",
      "Epoch 71/1000\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.5351 - acc: 0.7361 - val_loss: 0.5421 - val_acc: 0.7344\n",
      "Epoch 72/1000\n",
      "576/576 [==============================] - 0s 105us/step - loss: 0.5349 - acc: 0.7396 - val_loss: 0.5418 - val_acc: 0.7396\n",
      "Epoch 73/1000\n",
      "576/576 [==============================] - 0s 102us/step - loss: 0.5346 - acc: 0.7378 - val_loss: 0.5416 - val_acc: 0.7396\n",
      "Epoch 74/1000\n",
      "576/576 [==============================] - 0s 124us/step - loss: 0.5343 - acc: 0.7431 - val_loss: 0.5414 - val_acc: 0.7396\n",
      "Epoch 75/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.5341 - acc: 0.7431 - val_loss: 0.5411 - val_acc: 0.7396\n",
      "Epoch 76/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5338 - acc: 0.7431 - val_loss: 0.5409 - val_acc: 0.7396\n",
      "Epoch 77/1000\n",
      "576/576 [==============================] - 0s 103us/step - loss: 0.5336 - acc: 0.7431 - val_loss: 0.5407 - val_acc: 0.7396\n",
      "Epoch 78/1000\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5333 - acc: 0.7448 - val_loss: 0.5404 - val_acc: 0.7448\n",
      "Epoch 79/1000\n",
      "576/576 [==============================] - 0s 102us/step - loss: 0.5330 - acc: 0.7448 - val_loss: 0.5402 - val_acc: 0.7500\n",
      "Epoch 80/1000\n",
      "576/576 [==============================] - 0s 116us/step - loss: 0.5328 - acc: 0.7448 - val_loss: 0.5399 - val_acc: 0.7500\n",
      "Epoch 81/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5325 - acc: 0.7448 - val_loss: 0.5397 - val_acc: 0.7552\n",
      "Epoch 82/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5323 - acc: 0.7448 - val_loss: 0.5395 - val_acc: 0.7552\n",
      "Epoch 83/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5320 - acc: 0.7448 - val_loss: 0.5393 - val_acc: 0.7552\n",
      "Epoch 84/1000\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.5318 - acc: 0.7448 - val_loss: 0.5390 - val_acc: 0.7552\n",
      "Epoch 85/1000\n",
      "576/576 [==============================] - 0s 103us/step - loss: 0.5315 - acc: 0.7448 - val_loss: 0.5388 - val_acc: 0.7552\n",
      "Epoch 86/1000\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5313 - acc: 0.7448 - val_loss: 0.5386 - val_acc: 0.7552\n",
      "Epoch 87/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5310 - acc: 0.7448 - val_loss: 0.5383 - val_acc: 0.7552\n",
      "Epoch 88/1000\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.5308 - acc: 0.7448 - val_loss: 0.5381 - val_acc: 0.7552\n",
      "Epoch 89/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5305 - acc: 0.7431 - val_loss: 0.5379 - val_acc: 0.7552\n",
      "Epoch 90/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5303 - acc: 0.7431 - val_loss: 0.5377 - val_acc: 0.7552\n",
      "Epoch 91/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5301 - acc: 0.7431 - val_loss: 0.5374 - val_acc: 0.7552\n",
      "Epoch 92/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5298 - acc: 0.7465 - val_loss: 0.5372 - val_acc: 0.7552\n",
      "Epoch 93/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5295 - acc: 0.7465 - val_loss: 0.5370 - val_acc: 0.7604\n",
      "Epoch 94/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5293 - acc: 0.7465 - val_loss: 0.5368 - val_acc: 0.7552\n",
      "Epoch 95/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5291 - acc: 0.7448 - val_loss: 0.5365 - val_acc: 0.7552\n",
      "Epoch 96/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5288 - acc: 0.7431 - val_loss: 0.5363 - val_acc: 0.7552\n",
      "Epoch 97/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5286 - acc: 0.7465 - val_loss: 0.5361 - val_acc: 0.7552\n",
      "Epoch 98/1000\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.5283 - acc: 0.7483 - val_loss: 0.5359 - val_acc: 0.7552\n",
      "Epoch 99/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5281 - acc: 0.7413 - val_loss: 0.5357 - val_acc: 0.7552\n",
      "Epoch 100/1000\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5278 - acc: 0.7431 - val_loss: 0.5355 - val_acc: 0.7552\n",
      "Epoch 101/1000\n",
      "576/576 [==============================] - 0s 104us/step - loss: 0.5276 - acc: 0.7431 - val_loss: 0.5352 - val_acc: 0.7552\n",
      "Epoch 102/1000\n",
      "576/576 [==============================] - 0s 67us/step - loss: 0.5274 - acc: 0.7448 - val_loss: 0.5350 - val_acc: 0.7552\n",
      "Epoch 103/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5271 - acc: 0.7448 - val_loss: 0.5348 - val_acc: 0.7552\n",
      "Epoch 104/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5269 - acc: 0.7448 - val_loss: 0.5346 - val_acc: 0.7552\n",
      "Epoch 105/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5267 - acc: 0.7448 - val_loss: 0.5344 - val_acc: 0.7552\n",
      "Epoch 106/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5264 - acc: 0.7465 - val_loss: 0.5342 - val_acc: 0.7552\n",
      "Epoch 107/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5262 - acc: 0.7465 - val_loss: 0.5340 - val_acc: 0.7552\n",
      "Epoch 108/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5259 - acc: 0.7465 - val_loss: 0.5337 - val_acc: 0.7552\n",
      "Epoch 109/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5257 - acc: 0.7465 - val_loss: 0.5335 - val_acc: 0.7500\n",
      "Epoch 110/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5255 - acc: 0.7465 - val_loss: 0.5333 - val_acc: 0.7552\n",
      "Epoch 111/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5252 - acc: 0.7465 - val_loss: 0.5331 - val_acc: 0.7552\n",
      "Epoch 112/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5250 - acc: 0.7465 - val_loss: 0.5329 - val_acc: 0.7552\n",
      "Epoch 113/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5248 - acc: 0.7465 - val_loss: 0.5327 - val_acc: 0.7552\n",
      "Epoch 114/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5246 - acc: 0.7448 - val_loss: 0.5325 - val_acc: 0.7552\n",
      "Epoch 115/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5243 - acc: 0.7448 - val_loss: 0.5323 - val_acc: 0.7552\n",
      "Epoch 116/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5241 - acc: 0.7448 - val_loss: 0.5321 - val_acc: 0.7552\n",
      "Epoch 117/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5239 - acc: 0.7465 - val_loss: 0.5319 - val_acc: 0.7552\n",
      "Epoch 118/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5237 - acc: 0.7448 - val_loss: 0.5317 - val_acc: 0.7604\n",
      "Epoch 119/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5234 - acc: 0.7465 - val_loss: 0.5315 - val_acc: 0.7604\n",
      "Epoch 120/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5232 - acc: 0.7465 - val_loss: 0.5313 - val_acc: 0.7552\n",
      "Epoch 121/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5230 - acc: 0.7465 - val_loss: 0.5311 - val_acc: 0.7552\n",
      "Epoch 122/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5227 - acc: 0.7465 - val_loss: 0.5309 - val_acc: 0.7552\n",
      "Epoch 123/1000\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.5225 - acc: 0.7465 - val_loss: 0.5307 - val_acc: 0.7552\n",
      "Epoch 124/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5223 - acc: 0.7483 - val_loss: 0.5305 - val_acc: 0.7552\n",
      "Epoch 125/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5221 - acc: 0.7483 - val_loss: 0.5303 - val_acc: 0.7552\n",
      "Epoch 126/1000\n",
      "576/576 [==============================] - 0s 62us/step - loss: 0.5219 - acc: 0.7483 - val_loss: 0.5301 - val_acc: 0.7604\n",
      "Epoch 127/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5216 - acc: 0.7483 - val_loss: 0.5299 - val_acc: 0.7604\n",
      "Epoch 128/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5214 - acc: 0.7483 - val_loss: 0.5297 - val_acc: 0.7604\n",
      "Epoch 129/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5212 - acc: 0.7517 - val_loss: 0.5295 - val_acc: 0.7604\n",
      "Epoch 130/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5210 - acc: 0.7500 - val_loss: 0.5293 - val_acc: 0.7604\n",
      "Epoch 131/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5208 - acc: 0.7500 - val_loss: 0.5291 - val_acc: 0.7604\n",
      "Epoch 132/1000\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.5206 - acc: 0.7483 - val_loss: 0.5289 - val_acc: 0.7604\n",
      "Epoch 133/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5203 - acc: 0.7483 - val_loss: 0.5287 - val_acc: 0.7604\n",
      "Epoch 134/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5201 - acc: 0.7483 - val_loss: 0.5285 - val_acc: 0.7604\n",
      "Epoch 135/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5199 - acc: 0.7500 - val_loss: 0.5284 - val_acc: 0.7656\n",
      "Epoch 136/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5197 - acc: 0.7500 - val_loss: 0.5282 - val_acc: 0.7604\n",
      "Epoch 137/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5195 - acc: 0.7500 - val_loss: 0.5280 - val_acc: 0.7604\n",
      "Epoch 138/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.5193 - acc: 0.7483 - val_loss: 0.5278 - val_acc: 0.7604\n",
      "Epoch 139/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5191 - acc: 0.7500 - val_loss: 0.5276 - val_acc: 0.7604\n",
      "Epoch 140/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5188 - acc: 0.7483 - val_loss: 0.5274 - val_acc: 0.7604\n",
      "Epoch 141/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5186 - acc: 0.7500 - val_loss: 0.5272 - val_acc: 0.7604\n",
      "Epoch 142/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5184 - acc: 0.7483 - val_loss: 0.5271 - val_acc: 0.7604\n",
      "Epoch 143/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5182 - acc: 0.7483 - val_loss: 0.5269 - val_acc: 0.7604\n",
      "Epoch 144/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5180 - acc: 0.7483 - val_loss: 0.5267 - val_acc: 0.7604\n",
      "Epoch 145/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5178 - acc: 0.7500 - val_loss: 0.5265 - val_acc: 0.7552\n",
      "Epoch 146/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5176 - acc: 0.7483 - val_loss: 0.5263 - val_acc: 0.7552\n",
      "Epoch 147/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5174 - acc: 0.7500 - val_loss: 0.5261 - val_acc: 0.7552\n",
      "Epoch 148/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5172 - acc: 0.7500 - val_loss: 0.5260 - val_acc: 0.7552\n",
      "Epoch 149/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5170 - acc: 0.7500 - val_loss: 0.5258 - val_acc: 0.7552\n",
      "Epoch 150/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5168 - acc: 0.7483 - val_loss: 0.5256 - val_acc: 0.7552\n",
      "Epoch 151/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5166 - acc: 0.7483 - val_loss: 0.5254 - val_acc: 0.7552\n",
      "Epoch 152/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5164 - acc: 0.7483 - val_loss: 0.5253 - val_acc: 0.7552\n",
      "Epoch 153/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5162 - acc: 0.7483 - val_loss: 0.5251 - val_acc: 0.7552\n",
      "Epoch 154/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5159 - acc: 0.7483 - val_loss: 0.5249 - val_acc: 0.7552\n",
      "Epoch 155/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5157 - acc: 0.7483 - val_loss: 0.5247 - val_acc: 0.7552\n",
      "Epoch 156/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5156 - acc: 0.7483 - val_loss: 0.5246 - val_acc: 0.7552\n",
      "Epoch 157/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5154 - acc: 0.7483 - val_loss: 0.5244 - val_acc: 0.7552\n",
      "Epoch 158/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5152 - acc: 0.7465 - val_loss: 0.5242 - val_acc: 0.7552\n",
      "Epoch 159/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5150 - acc: 0.7483 - val_loss: 0.5240 - val_acc: 0.7552\n",
      "Epoch 160/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5147 - acc: 0.7483 - val_loss: 0.5239 - val_acc: 0.7552\n",
      "Epoch 161/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5146 - acc: 0.7483 - val_loss: 0.5237 - val_acc: 0.7552\n",
      "Epoch 162/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5144 - acc: 0.7465 - val_loss: 0.5235 - val_acc: 0.7500\n",
      "Epoch 163/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5142 - acc: 0.7483 - val_loss: 0.5234 - val_acc: 0.7500\n",
      "Epoch 164/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5140 - acc: 0.7465 - val_loss: 0.5232 - val_acc: 0.7500\n",
      "Epoch 165/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5138 - acc: 0.7465 - val_loss: 0.5230 - val_acc: 0.7500\n",
      "Epoch 166/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5136 - acc: 0.7483 - val_loss: 0.5228 - val_acc: 0.7500\n",
      "Epoch 167/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5134 - acc: 0.7483 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 168/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5132 - acc: 0.7483 - val_loss: 0.5225 - val_acc: 0.7500\n",
      "Epoch 169/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5130 - acc: 0.7483 - val_loss: 0.5224 - val_acc: 0.7500\n",
      "Epoch 170/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5128 - acc: 0.7483 - val_loss: 0.5222 - val_acc: 0.7500\n",
      "Epoch 171/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5126 - acc: 0.7483 - val_loss: 0.5220 - val_acc: 0.7500\n",
      "Epoch 172/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5124 - acc: 0.7483 - val_loss: 0.5219 - val_acc: 0.7500\n",
      "Epoch 173/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5123 - acc: 0.7500 - val_loss: 0.5217 - val_acc: 0.7500\n",
      "Epoch 174/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5121 - acc: 0.7517 - val_loss: 0.5215 - val_acc: 0.7552\n",
      "Epoch 175/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5119 - acc: 0.7517 - val_loss: 0.5214 - val_acc: 0.7552\n",
      "Epoch 176/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4547 - acc: 0.812 - 0s 57us/step - loss: 0.5117 - acc: 0.7517 - val_loss: 0.5212 - val_acc: 0.7552\n",
      "Epoch 177/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5115 - acc: 0.7517 - val_loss: 0.5211 - val_acc: 0.7552\n",
      "Epoch 178/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5113 - acc: 0.7517 - val_loss: 0.5209 - val_acc: 0.7552\n",
      "Epoch 179/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5745 - acc: 0.687 - 0s 54us/step - loss: 0.5111 - acc: 0.7517 - val_loss: 0.5207 - val_acc: 0.7552\n",
      "Epoch 180/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 59us/step - loss: 0.5110 - acc: 0.7517 - val_loss: 0.5206 - val_acc: 0.7552\n",
      "Epoch 181/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5108 - acc: 0.7517 - val_loss: 0.5204 - val_acc: 0.7552\n",
      "Epoch 182/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5106 - acc: 0.7517 - val_loss: 0.5203 - val_acc: 0.7552\n",
      "Epoch 183/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5104 - acc: 0.7517 - val_loss: 0.5201 - val_acc: 0.7552\n",
      "Epoch 184/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5102 - acc: 0.7517 - val_loss: 0.5200 - val_acc: 0.7552\n",
      "Epoch 185/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5100 - acc: 0.7517 - val_loss: 0.5198 - val_acc: 0.7552\n",
      "Epoch 186/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5099 - acc: 0.7517 - val_loss: 0.5197 - val_acc: 0.7552\n",
      "Epoch 187/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5097 - acc: 0.7517 - val_loss: 0.5195 - val_acc: 0.7552\n",
      "Epoch 188/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5095 - acc: 0.7517 - val_loss: 0.5193 - val_acc: 0.7552\n",
      "Epoch 189/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5093 - acc: 0.7500 - val_loss: 0.5192 - val_acc: 0.7604\n",
      "Epoch 190/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5091 - acc: 0.7517 - val_loss: 0.5190 - val_acc: 0.7604\n",
      "Epoch 191/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.5090 - acc: 0.7517 - val_loss: 0.5189 - val_acc: 0.7604\n",
      "Epoch 192/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5088 - acc: 0.7500 - val_loss: 0.5187 - val_acc: 0.7604\n",
      "Epoch 193/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5086 - acc: 0.7500 - val_loss: 0.5186 - val_acc: 0.7604\n",
      "Epoch 194/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5085 - acc: 0.7500 - val_loss: 0.5184 - val_acc: 0.7604\n",
      "Epoch 195/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5083 - acc: 0.7517 - val_loss: 0.5183 - val_acc: 0.7604\n",
      "Epoch 196/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5081 - acc: 0.7500 - val_loss: 0.5182 - val_acc: 0.7604\n",
      "Epoch 197/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5079 - acc: 0.7500 - val_loss: 0.5180 - val_acc: 0.7604\n",
      "Epoch 198/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5077 - acc: 0.7500 - val_loss: 0.5179 - val_acc: 0.7656\n",
      "Epoch 199/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5076 - acc: 0.7500 - val_loss: 0.5177 - val_acc: 0.7656\n",
      "Epoch 200/1000\n",
      "576/576 [==============================] - 0s 109us/step - loss: 0.5074 - acc: 0.7483 - val_loss: 0.5176 - val_acc: 0.7656\n",
      "Epoch 201/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5072 - acc: 0.7500 - val_loss: 0.5174 - val_acc: 0.7656\n",
      "Epoch 202/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5070 - acc: 0.7500 - val_loss: 0.5173 - val_acc: 0.7656\n",
      "Epoch 203/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5069 - acc: 0.7500 - val_loss: 0.5171 - val_acc: 0.7656\n",
      "Epoch 204/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5067 - acc: 0.7500 - val_loss: 0.5170 - val_acc: 0.7656\n",
      "Epoch 205/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5065 - acc: 0.7500 - val_loss: 0.5169 - val_acc: 0.7656\n",
      "Epoch 206/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5064 - acc: 0.7500 - val_loss: 0.5167 - val_acc: 0.7656\n",
      "Epoch 207/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5062 - acc: 0.7500 - val_loss: 0.5166 - val_acc: 0.7656\n",
      "Epoch 208/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5060 - acc: 0.7500 - val_loss: 0.5164 - val_acc: 0.7656\n",
      "Epoch 209/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5059 - acc: 0.7500 - val_loss: 0.5163 - val_acc: 0.7656\n",
      "Epoch 210/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5057 - acc: 0.7500 - val_loss: 0.5162 - val_acc: 0.7656\n",
      "Epoch 211/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5055 - acc: 0.7500 - val_loss: 0.5160 - val_acc: 0.7656\n",
      "Epoch 212/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.5054 - acc: 0.7500 - val_loss: 0.5159 - val_acc: 0.7656\n",
      "Epoch 213/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5052 - acc: 0.7500 - val_loss: 0.5157 - val_acc: 0.7656\n",
      "Epoch 214/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5050 - acc: 0.7500 - val_loss: 0.5156 - val_acc: 0.7656\n",
      "Epoch 215/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.5049 - acc: 0.7517 - val_loss: 0.5155 - val_acc: 0.7656\n",
      "Epoch 216/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5047 - acc: 0.7500 - val_loss: 0.5153 - val_acc: 0.7656\n",
      "Epoch 217/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5045 - acc: 0.7500 - val_loss: 0.5152 - val_acc: 0.7656\n",
      "Epoch 218/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.5044 - acc: 0.7500 - val_loss: 0.5151 - val_acc: 0.7656\n",
      "Epoch 219/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5042 - acc: 0.7500 - val_loss: 0.5149 - val_acc: 0.7656\n",
      "Epoch 220/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5041 - acc: 0.7535 - val_loss: 0.5148 - val_acc: 0.7656\n",
      "Epoch 221/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5039 - acc: 0.7535 - val_loss: 0.5147 - val_acc: 0.7656\n",
      "Epoch 222/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5037 - acc: 0.7535 - val_loss: 0.5145 - val_acc: 0.7656\n",
      "Epoch 223/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.5036 - acc: 0.7535 - val_loss: 0.5144 - val_acc: 0.7656\n",
      "Epoch 224/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.5034 - acc: 0.7535 - val_loss: 0.5143 - val_acc: 0.7656\n",
      "Epoch 225/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.5033 - acc: 0.7535 - val_loss: 0.5141 - val_acc: 0.7656\n",
      "Epoch 226/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5031 - acc: 0.7535 - val_loss: 0.5140 - val_acc: 0.7656\n",
      "Epoch 227/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5029 - acc: 0.7535 - val_loss: 0.5139 - val_acc: 0.7656\n",
      "Epoch 228/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.5028 - acc: 0.7552 - val_loss: 0.5138 - val_acc: 0.7656\n",
      "Epoch 229/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.5026 - acc: 0.7552 - val_loss: 0.5136 - val_acc: 0.7708\n",
      "Epoch 230/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.5025 - acc: 0.7552 - val_loss: 0.5135 - val_acc: 0.7656\n",
      "Epoch 231/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.5023 - acc: 0.7552 - val_loss: 0.5134 - val_acc: 0.7656\n",
      "Epoch 232/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5022 - acc: 0.7552 - val_loss: 0.5133 - val_acc: 0.7656\n",
      "Epoch 233/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5020 - acc: 0.7552 - val_loss: 0.5131 - val_acc: 0.7708\n",
      "Epoch 234/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.5019 - acc: 0.7552 - val_loss: 0.5130 - val_acc: 0.7760\n",
      "Epoch 235/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.5017 - acc: 0.7569 - val_loss: 0.5129 - val_acc: 0.7760\n",
      "Epoch 236/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.5016 - acc: 0.7569 - val_loss: 0.5128 - val_acc: 0.7760\n",
      "Epoch 237/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.5014 - acc: 0.7569 - val_loss: 0.5126 - val_acc: 0.7760\n",
      "Epoch 238/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.5012 - acc: 0.7569 - val_loss: 0.5125 - val_acc: 0.7760\n",
      "Epoch 239/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5011 - acc: 0.7569 - val_loss: 0.5124 - val_acc: 0.7760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.5009 - acc: 0.7569 - val_loss: 0.5123 - val_acc: 0.7760\n",
      "Epoch 241/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5008 - acc: 0.7569 - val_loss: 0.5121 - val_acc: 0.7760\n",
      "Epoch 242/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5007 - acc: 0.7569 - val_loss: 0.5120 - val_acc: 0.7760\n",
      "Epoch 243/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.5005 - acc: 0.7569 - val_loss: 0.5119 - val_acc: 0.7760\n",
      "Epoch 244/1000\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.5004 - acc: 0.7569 - val_loss: 0.5118 - val_acc: 0.7760\n",
      "Epoch 245/1000\n",
      "576/576 [==============================] - 0s 141us/step - loss: 0.5002 - acc: 0.7569 - val_loss: 0.5117 - val_acc: 0.7760\n",
      "Epoch 246/1000\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5001 - acc: 0.7552 - val_loss: 0.5116 - val_acc: 0.7760\n",
      "Epoch 247/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4999 - acc: 0.7569 - val_loss: 0.5114 - val_acc: 0.7760\n",
      "Epoch 248/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4998 - acc: 0.7569 - val_loss: 0.5113 - val_acc: 0.7760\n",
      "Epoch 249/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4996 - acc: 0.7587 - val_loss: 0.5112 - val_acc: 0.7760\n",
      "Epoch 250/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4995 - acc: 0.7587 - val_loss: 0.5111 - val_acc: 0.7760\n",
      "Epoch 251/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4993 - acc: 0.7587 - val_loss: 0.5110 - val_acc: 0.7760\n",
      "Epoch 252/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4992 - acc: 0.7587 - val_loss: 0.5109 - val_acc: 0.7760\n",
      "Epoch 253/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4991 - acc: 0.7587 - val_loss: 0.5107 - val_acc: 0.7760\n",
      "Epoch 254/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4989 - acc: 0.7587 - val_loss: 0.5106 - val_acc: 0.7760\n",
      "Epoch 255/1000\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4988 - acc: 0.7604 - val_loss: 0.5105 - val_acc: 0.7760\n",
      "Epoch 256/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4986 - acc: 0.7604 - val_loss: 0.5104 - val_acc: 0.7760\n",
      "Epoch 257/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4985 - acc: 0.7604 - val_loss: 0.5103 - val_acc: 0.7760\n",
      "Epoch 258/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4984 - acc: 0.7604 - val_loss: 0.5102 - val_acc: 0.7760\n",
      "Epoch 259/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4982 - acc: 0.7604 - val_loss: 0.5101 - val_acc: 0.7760\n",
      "Epoch 260/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4981 - acc: 0.7604 - val_loss: 0.5100 - val_acc: 0.7760\n",
      "Epoch 261/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4979 - acc: 0.7604 - val_loss: 0.5098 - val_acc: 0.7760\n",
      "Epoch 262/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4978 - acc: 0.7604 - val_loss: 0.5097 - val_acc: 0.7760\n",
      "Epoch 263/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4977 - acc: 0.7604 - val_loss: 0.5096 - val_acc: 0.7760\n",
      "Epoch 264/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4975 - acc: 0.7604 - val_loss: 0.5095 - val_acc: 0.7760\n",
      "Epoch 265/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4974 - acc: 0.7604 - val_loss: 0.5094 - val_acc: 0.7760\n",
      "Epoch 266/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4972 - acc: 0.7604 - val_loss: 0.5093 - val_acc: 0.7760\n",
      "Epoch 267/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4971 - acc: 0.7604 - val_loss: 0.5092 - val_acc: 0.7760\n",
      "Epoch 268/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4970 - acc: 0.7604 - val_loss: 0.5091 - val_acc: 0.7760\n",
      "Epoch 269/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4968 - acc: 0.7604 - val_loss: 0.5090 - val_acc: 0.7760\n",
      "Epoch 270/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4967 - acc: 0.7604 - val_loss: 0.5089 - val_acc: 0.7760\n",
      "Epoch 271/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4965 - acc: 0.7604 - val_loss: 0.5088 - val_acc: 0.7760\n",
      "Epoch 272/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4964 - acc: 0.7604 - val_loss: 0.5087 - val_acc: 0.7760\n",
      "Epoch 273/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4963 - acc: 0.7604 - val_loss: 0.5086 - val_acc: 0.7760\n",
      "Epoch 274/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4961 - acc: 0.7604 - val_loss: 0.5084 - val_acc: 0.7760\n",
      "Epoch 275/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4960 - acc: 0.7604 - val_loss: 0.5083 - val_acc: 0.7760\n",
      "Epoch 276/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4959 - acc: 0.7604 - val_loss: 0.5082 - val_acc: 0.7760\n",
      "Epoch 277/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4957 - acc: 0.7604 - val_loss: 0.5081 - val_acc: 0.7760\n",
      "Epoch 278/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4956 - acc: 0.7604 - val_loss: 0.5080 - val_acc: 0.7760\n",
      "Epoch 279/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4955 - acc: 0.7604 - val_loss: 0.5079 - val_acc: 0.7760\n",
      "Epoch 280/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4954 - acc: 0.7604 - val_loss: 0.5078 - val_acc: 0.7760\n",
      "Epoch 281/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4952 - acc: 0.7604 - val_loss: 0.5077 - val_acc: 0.7760\n",
      "Epoch 282/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4951 - acc: 0.7604 - val_loss: 0.5076 - val_acc: 0.7760\n",
      "Epoch 283/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4950 - acc: 0.7604 - val_loss: 0.5075 - val_acc: 0.7760\n",
      "Epoch 284/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4949 - acc: 0.7604 - val_loss: 0.5074 - val_acc: 0.7760\n",
      "Epoch 285/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4947 - acc: 0.7604 - val_loss: 0.5073 - val_acc: 0.7760\n",
      "Epoch 286/1000\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.4946 - acc: 0.7604 - val_loss: 0.5072 - val_acc: 0.7760\n",
      "Epoch 287/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4945 - acc: 0.7604 - val_loss: 0.5071 - val_acc: 0.7760\n",
      "Epoch 288/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4943 - acc: 0.7604 - val_loss: 0.5070 - val_acc: 0.7760\n",
      "Epoch 289/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4942 - acc: 0.7604 - val_loss: 0.5069 - val_acc: 0.7760\n",
      "Epoch 290/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4941 - acc: 0.7604 - val_loss: 0.5068 - val_acc: 0.7812\n",
      "Epoch 291/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4940 - acc: 0.7604 - val_loss: 0.5067 - val_acc: 0.7812\n",
      "Epoch 292/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4938 - acc: 0.7604 - val_loss: 0.5066 - val_acc: 0.7812\n",
      "Epoch 293/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4937 - acc: 0.7604 - val_loss: 0.5066 - val_acc: 0.7812\n",
      "Epoch 294/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4936 - acc: 0.7604 - val_loss: 0.5065 - val_acc: 0.7812\n",
      "Epoch 295/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4935 - acc: 0.7604 - val_loss: 0.5064 - val_acc: 0.7812\n",
      "Epoch 296/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4933 - acc: 0.7604 - val_loss: 0.5063 - val_acc: 0.7812\n",
      "Epoch 297/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4932 - acc: 0.7604 - val_loss: 0.5062 - val_acc: 0.7812\n",
      "Epoch 298/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4931 - acc: 0.7604 - val_loss: 0.5061 - val_acc: 0.7812\n",
      "Epoch 299/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4930 - acc: 0.7604 - val_loss: 0.5060 - val_acc: 0.7812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4928 - acc: 0.7587 - val_loss: 0.5059 - val_acc: 0.7812\n",
      "Epoch 301/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4927 - acc: 0.7587 - val_loss: 0.5058 - val_acc: 0.7812\n",
      "Epoch 302/1000\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.4926 - acc: 0.7604 - val_loss: 0.5057 - val_acc: 0.7812\n",
      "Epoch 303/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4925 - acc: 0.7604 - val_loss: 0.5056 - val_acc: 0.7812\n",
      "Epoch 304/1000\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4924 - acc: 0.7587 - val_loss: 0.5055 - val_acc: 0.7812\n",
      "Epoch 305/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4923 - acc: 0.7587 - val_loss: 0.5054 - val_acc: 0.7812\n",
      "Epoch 306/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4921 - acc: 0.7587 - val_loss: 0.5053 - val_acc: 0.7812\n",
      "Epoch 307/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4920 - acc: 0.7587 - val_loss: 0.5053 - val_acc: 0.7812\n",
      "Epoch 308/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4919 - acc: 0.7587 - val_loss: 0.5052 - val_acc: 0.7812\n",
      "Epoch 309/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4918 - acc: 0.7587 - val_loss: 0.5051 - val_acc: 0.7812\n",
      "Epoch 310/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4916 - acc: 0.7587 - val_loss: 0.5050 - val_acc: 0.7812\n",
      "Epoch 311/1000\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4915 - acc: 0.7569 - val_loss: 0.5049 - val_acc: 0.7812\n",
      "Epoch 312/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4914 - acc: 0.7587 - val_loss: 0.5048 - val_acc: 0.7812\n",
      "Epoch 313/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4913 - acc: 0.7569 - val_loss: 0.5047 - val_acc: 0.7812\n",
      "Epoch 314/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4912 - acc: 0.7604 - val_loss: 0.5046 - val_acc: 0.7760\n",
      "Epoch 315/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4911 - acc: 0.7587 - val_loss: 0.5046 - val_acc: 0.7708\n",
      "Epoch 316/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4910 - acc: 0.7587 - val_loss: 0.5045 - val_acc: 0.7708\n",
      "Epoch 317/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4908 - acc: 0.7587 - val_loss: 0.5044 - val_acc: 0.7708\n",
      "Epoch 318/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4907 - acc: 0.7604 - val_loss: 0.5043 - val_acc: 0.7708\n",
      "Epoch 319/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4906 - acc: 0.7587 - val_loss: 0.5042 - val_acc: 0.7708\n",
      "Epoch 320/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4905 - acc: 0.7587 - val_loss: 0.5041 - val_acc: 0.7708\n",
      "Epoch 321/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4904 - acc: 0.7587 - val_loss: 0.5040 - val_acc: 0.7708\n",
      "Epoch 322/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4903 - acc: 0.7587 - val_loss: 0.5040 - val_acc: 0.7708\n",
      "Epoch 323/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4901 - acc: 0.7587 - val_loss: 0.5039 - val_acc: 0.7708\n",
      "Epoch 324/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4900 - acc: 0.7587 - val_loss: 0.5038 - val_acc: 0.7708\n",
      "Epoch 325/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4900 - acc: 0.7587 - val_loss: 0.5037 - val_acc: 0.7708\n",
      "Epoch 326/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4898 - acc: 0.7587 - val_loss: 0.5036 - val_acc: 0.7708\n",
      "Epoch 327/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4897 - acc: 0.7587 - val_loss: 0.5035 - val_acc: 0.7708\n",
      "Epoch 328/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4896 - acc: 0.7587 - val_loss: 0.5035 - val_acc: 0.7760\n",
      "Epoch 329/1000\n",
      "576/576 [==============================] - 0s 101us/step - loss: 0.4895 - acc: 0.7587 - val_loss: 0.5034 - val_acc: 0.7760\n",
      "Epoch 330/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4894 - acc: 0.7587 - val_loss: 0.5033 - val_acc: 0.7760\n",
      "Epoch 331/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4893 - acc: 0.7587 - val_loss: 0.5032 - val_acc: 0.7760\n",
      "Epoch 332/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4892 - acc: 0.7587 - val_loss: 0.5031 - val_acc: 0.7760\n",
      "Epoch 333/1000\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4891 - acc: 0.7587 - val_loss: 0.5031 - val_acc: 0.7760\n",
      "Epoch 334/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4890 - acc: 0.7587 - val_loss: 0.5030 - val_acc: 0.7760\n",
      "Epoch 335/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4888 - acc: 0.7587 - val_loss: 0.5029 - val_acc: 0.7760\n",
      "Epoch 336/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4887 - acc: 0.7587 - val_loss: 0.5028 - val_acc: 0.7760\n",
      "Epoch 337/1000\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4886 - acc: 0.7604 - val_loss: 0.5027 - val_acc: 0.7760\n",
      "Epoch 338/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4885 - acc: 0.7587 - val_loss: 0.5027 - val_acc: 0.7760\n",
      "Epoch 339/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4884 - acc: 0.7604 - val_loss: 0.5026 - val_acc: 0.7708\n",
      "Epoch 340/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4883 - acc: 0.7604 - val_loss: 0.5025 - val_acc: 0.7708\n",
      "Epoch 341/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4882 - acc: 0.7604 - val_loss: 0.5024 - val_acc: 0.7708\n",
      "Epoch 342/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4881 - acc: 0.7622 - val_loss: 0.5024 - val_acc: 0.7708\n",
      "Epoch 343/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4880 - acc: 0.7622 - val_loss: 0.5023 - val_acc: 0.7708\n",
      "Epoch 344/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4879 - acc: 0.7622 - val_loss: 0.5022 - val_acc: 0.7708\n",
      "Epoch 345/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4878 - acc: 0.7622 - val_loss: 0.5021 - val_acc: 0.7708\n",
      "Epoch 346/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4877 - acc: 0.7622 - val_loss: 0.5021 - val_acc: 0.7708\n",
      "Epoch 347/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4876 - acc: 0.7622 - val_loss: 0.5020 - val_acc: 0.7708\n",
      "Epoch 348/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4875 - acc: 0.7622 - val_loss: 0.5019 - val_acc: 0.7708\n",
      "Epoch 349/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4874 - acc: 0.7622 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 350/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4873 - acc: 0.7622 - val_loss: 0.5018 - val_acc: 0.7708\n",
      "Epoch 351/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4872 - acc: 0.7622 - val_loss: 0.5017 - val_acc: 0.7708\n",
      "Epoch 352/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4871 - acc: 0.7622 - val_loss: 0.5016 - val_acc: 0.7708\n",
      "Epoch 353/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4869 - acc: 0.7622 - val_loss: 0.5015 - val_acc: 0.7708\n",
      "Epoch 354/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4869 - acc: 0.7622 - val_loss: 0.5015 - val_acc: 0.7708\n",
      "Epoch 355/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4867 - acc: 0.7622 - val_loss: 0.5014 - val_acc: 0.7708\n",
      "Epoch 356/1000\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4866 - acc: 0.7622 - val_loss: 0.5013 - val_acc: 0.7708\n",
      "Epoch 357/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4865 - acc: 0.7622 - val_loss: 0.5013 - val_acc: 0.7708\n",
      "Epoch 358/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4864 - acc: 0.7622 - val_loss: 0.5012 - val_acc: 0.7708\n",
      "Epoch 359/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4864 - acc: 0.7622 - val_loss: 0.5011 - val_acc: 0.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4863 - acc: 0.7622 - val_loss: 0.5010 - val_acc: 0.7708\n",
      "Epoch 361/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4861 - acc: 0.7622 - val_loss: 0.5010 - val_acc: 0.7708\n",
      "Epoch 362/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4860 - acc: 0.7622 - val_loss: 0.5009 - val_acc: 0.7708\n",
      "Epoch 363/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4860 - acc: 0.7639 - val_loss: 0.5008 - val_acc: 0.7708\n",
      "Epoch 364/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4763 - acc: 0.750 - 0s 82us/step - loss: 0.4859 - acc: 0.7639 - val_loss: 0.5008 - val_acc: 0.7708\n",
      "Epoch 365/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4858 - acc: 0.7639 - val_loss: 0.5007 - val_acc: 0.7708\n",
      "Epoch 366/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4857 - acc: 0.7639 - val_loss: 0.5006 - val_acc: 0.7708\n",
      "Epoch 367/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4856 - acc: 0.7639 - val_loss: 0.5006 - val_acc: 0.7708\n",
      "Epoch 368/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4855 - acc: 0.7639 - val_loss: 0.5005 - val_acc: 0.7708\n",
      "Epoch 369/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4854 - acc: 0.7639 - val_loss: 0.5004 - val_acc: 0.7708\n",
      "Epoch 370/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4853 - acc: 0.7656 - val_loss: 0.5004 - val_acc: 0.7708\n",
      "Epoch 371/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4852 - acc: 0.7656 - val_loss: 0.5003 - val_acc: 0.7708\n",
      "Epoch 372/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4851 - acc: 0.7656 - val_loss: 0.5002 - val_acc: 0.7708\n",
      "Epoch 373/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4850 - acc: 0.7639 - val_loss: 0.5002 - val_acc: 0.7708\n",
      "Epoch 374/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4849 - acc: 0.7639 - val_loss: 0.5001 - val_acc: 0.7708\n",
      "Epoch 375/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4848 - acc: 0.7639 - val_loss: 0.5000 - val_acc: 0.7708\n",
      "Epoch 376/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4847 - acc: 0.7639 - val_loss: 0.5000 - val_acc: 0.7708\n",
      "Epoch 377/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4846 - acc: 0.7639 - val_loss: 0.4999 - val_acc: 0.7708\n",
      "Epoch 378/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4845 - acc: 0.7622 - val_loss: 0.4998 - val_acc: 0.7708\n",
      "Epoch 379/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4844 - acc: 0.7639 - val_loss: 0.4998 - val_acc: 0.7656\n",
      "Epoch 380/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4844 - acc: 0.7639 - val_loss: 0.4997 - val_acc: 0.7656\n",
      "Epoch 381/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4842 - acc: 0.7639 - val_loss: 0.4996 - val_acc: 0.7656\n",
      "Epoch 382/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4842 - acc: 0.7622 - val_loss: 0.4996 - val_acc: 0.7656\n",
      "Epoch 383/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4841 - acc: 0.7639 - val_loss: 0.4995 - val_acc: 0.7656\n",
      "Epoch 384/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4840 - acc: 0.7622 - val_loss: 0.4994 - val_acc: 0.7656\n",
      "Epoch 385/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4839 - acc: 0.7622 - val_loss: 0.4994 - val_acc: 0.7656\n",
      "Epoch 386/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4838 - acc: 0.7622 - val_loss: 0.4993 - val_acc: 0.7656\n",
      "Epoch 387/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4837 - acc: 0.7622 - val_loss: 0.4993 - val_acc: 0.7656\n",
      "Epoch 388/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4836 - acc: 0.7622 - val_loss: 0.4992 - val_acc: 0.7656\n",
      "Epoch 389/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4835 - acc: 0.7622 - val_loss: 0.4991 - val_acc: 0.7656\n",
      "Epoch 390/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4834 - acc: 0.7622 - val_loss: 0.4991 - val_acc: 0.7656\n",
      "Epoch 391/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4833 - acc: 0.7622 - val_loss: 0.4990 - val_acc: 0.7656\n",
      "Epoch 392/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4833 - acc: 0.7622 - val_loss: 0.4990 - val_acc: 0.7656\n",
      "Epoch 393/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4832 - acc: 0.7622 - val_loss: 0.4989 - val_acc: 0.7656\n",
      "Epoch 394/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4831 - acc: 0.7622 - val_loss: 0.4988 - val_acc: 0.7656\n",
      "Epoch 395/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4830 - acc: 0.7622 - val_loss: 0.4988 - val_acc: 0.7656\n",
      "Epoch 396/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4829 - acc: 0.7622 - val_loss: 0.4987 - val_acc: 0.7656\n",
      "Epoch 397/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4828 - acc: 0.7639 - val_loss: 0.4987 - val_acc: 0.7656\n",
      "Epoch 398/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4827 - acc: 0.7639 - val_loss: 0.4986 - val_acc: 0.7656\n",
      "Epoch 399/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4826 - acc: 0.7639 - val_loss: 0.4985 - val_acc: 0.7656\n",
      "Epoch 400/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4826 - acc: 0.7639 - val_loss: 0.4985 - val_acc: 0.7656\n",
      "Epoch 401/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4825 - acc: 0.7639 - val_loss: 0.4984 - val_acc: 0.7656\n",
      "Epoch 402/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4824 - acc: 0.7639 - val_loss: 0.4984 - val_acc: 0.7656\n",
      "Epoch 403/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4823 - acc: 0.7639 - val_loss: 0.4983 - val_acc: 0.7656\n",
      "Epoch 404/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4822 - acc: 0.7639 - val_loss: 0.4982 - val_acc: 0.7656\n",
      "Epoch 405/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4821 - acc: 0.7639 - val_loss: 0.4982 - val_acc: 0.7656\n",
      "Epoch 406/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4820 - acc: 0.7656 - val_loss: 0.4981 - val_acc: 0.7656\n",
      "Epoch 407/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4820 - acc: 0.7656 - val_loss: 0.4981 - val_acc: 0.7656\n",
      "Epoch 408/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4819 - acc: 0.7656 - val_loss: 0.4980 - val_acc: 0.7656\n",
      "Epoch 409/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4818 - acc: 0.7656 - val_loss: 0.4980 - val_acc: 0.7656\n",
      "Epoch 410/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4817 - acc: 0.7656 - val_loss: 0.4979 - val_acc: 0.7656\n",
      "Epoch 411/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4816 - acc: 0.7656 - val_loss: 0.4979 - val_acc: 0.7656\n",
      "Epoch 412/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4815 - acc: 0.7656 - val_loss: 0.4978 - val_acc: 0.7656\n",
      "Epoch 413/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4815 - acc: 0.7656 - val_loss: 0.4977 - val_acc: 0.7656\n",
      "Epoch 414/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4814 - acc: 0.7656 - val_loss: 0.4977 - val_acc: 0.7656\n",
      "Epoch 415/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4813 - acc: 0.7656 - val_loss: 0.4976 - val_acc: 0.7656\n",
      "Epoch 416/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4812 - acc: 0.7656 - val_loss: 0.4976 - val_acc: 0.7656\n",
      "Epoch 417/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4811 - acc: 0.7656 - val_loss: 0.4975 - val_acc: 0.7656\n",
      "Epoch 418/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4810 - acc: 0.7656 - val_loss: 0.4975 - val_acc: 0.7656\n",
      "Epoch 419/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4810 - acc: 0.7656 - val_loss: 0.4974 - val_acc: 0.7656\n",
      "Epoch 420/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4809 - acc: 0.7656 - val_loss: 0.4974 - val_acc: 0.7656\n",
      "Epoch 421/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4808 - acc: 0.7656 - val_loss: 0.4973 - val_acc: 0.7656\n",
      "Epoch 422/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4807 - acc: 0.7656 - val_loss: 0.4973 - val_acc: 0.7656\n",
      "Epoch 423/1000\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4806 - acc: 0.7656 - val_loss: 0.4972 - val_acc: 0.7656\n",
      "Epoch 424/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4806 - acc: 0.7656 - val_loss: 0.4972 - val_acc: 0.7656\n",
      "Epoch 425/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4805 - acc: 0.7656 - val_loss: 0.4971 - val_acc: 0.7656\n",
      "Epoch 426/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4804 - acc: 0.7674 - val_loss: 0.4971 - val_acc: 0.7656\n",
      "Epoch 427/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4803 - acc: 0.7656 - val_loss: 0.4970 - val_acc: 0.7656\n",
      "Epoch 428/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4802 - acc: 0.7656 - val_loss: 0.4970 - val_acc: 0.7656\n",
      "Epoch 429/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4802 - acc: 0.7639 - val_loss: 0.4969 - val_acc: 0.7656\n",
      "Epoch 430/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4801 - acc: 0.7656 - val_loss: 0.4969 - val_acc: 0.7656\n",
      "Epoch 431/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4800 - acc: 0.7674 - val_loss: 0.4968 - val_acc: 0.7656\n",
      "Epoch 432/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4799 - acc: 0.7656 - val_loss: 0.4967 - val_acc: 0.7656\n",
      "Epoch 433/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4799 - acc: 0.7656 - val_loss: 0.4967 - val_acc: 0.7656\n",
      "Epoch 434/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4798 - acc: 0.7674 - val_loss: 0.4967 - val_acc: 0.7656\n",
      "Epoch 435/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4797 - acc: 0.7691 - val_loss: 0.4966 - val_acc: 0.7656\n",
      "Epoch 436/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4796 - acc: 0.7656 - val_loss: 0.4966 - val_acc: 0.7656\n",
      "Epoch 437/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4796 - acc: 0.7691 - val_loss: 0.4965 - val_acc: 0.7656\n",
      "Epoch 438/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4795 - acc: 0.7656 - val_loss: 0.4965 - val_acc: 0.7656\n",
      "Epoch 439/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4794 - acc: 0.7674 - val_loss: 0.4964 - val_acc: 0.7656\n",
      "Epoch 440/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4793 - acc: 0.7656 - val_loss: 0.4964 - val_acc: 0.7656\n",
      "Epoch 441/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4792 - acc: 0.7674 - val_loss: 0.4963 - val_acc: 0.7656\n",
      "Epoch 442/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4792 - acc: 0.7656 - val_loss: 0.4963 - val_acc: 0.7656\n",
      "Epoch 443/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4791 - acc: 0.7656 - val_loss: 0.4962 - val_acc: 0.7656\n",
      "Epoch 444/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4790 - acc: 0.7656 - val_loss: 0.4962 - val_acc: 0.7656\n",
      "Epoch 445/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4790 - acc: 0.7656 - val_loss: 0.4961 - val_acc: 0.7656\n",
      "Epoch 446/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4789 - acc: 0.7656 - val_loss: 0.4961 - val_acc: 0.7656\n",
      "Epoch 447/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4788 - acc: 0.7656 - val_loss: 0.4960 - val_acc: 0.7656\n",
      "Epoch 448/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4787 - acc: 0.7656 - val_loss: 0.4960 - val_acc: 0.7656\n",
      "Epoch 449/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4787 - acc: 0.7656 - val_loss: 0.4959 - val_acc: 0.7656\n",
      "Epoch 450/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4786 - acc: 0.7656 - val_loss: 0.4959 - val_acc: 0.7656\n",
      "Epoch 451/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4785 - acc: 0.7656 - val_loss: 0.4958 - val_acc: 0.7656\n",
      "Epoch 452/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4784 - acc: 0.7656 - val_loss: 0.4958 - val_acc: 0.7656\n",
      "Epoch 453/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4784 - acc: 0.7656 - val_loss: 0.4958 - val_acc: 0.7656\n",
      "Epoch 454/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4783 - acc: 0.7674 - val_loss: 0.4957 - val_acc: 0.7656\n",
      "Epoch 455/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4782 - acc: 0.7656 - val_loss: 0.4957 - val_acc: 0.7656\n",
      "Epoch 456/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4781 - acc: 0.7656 - val_loss: 0.4956 - val_acc: 0.7656\n",
      "Epoch 457/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4781 - acc: 0.7656 - val_loss: 0.4956 - val_acc: 0.7656\n",
      "Epoch 458/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4780 - acc: 0.7656 - val_loss: 0.4955 - val_acc: 0.7656\n",
      "Epoch 459/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4779 - acc: 0.7656 - val_loss: 0.4955 - val_acc: 0.7656\n",
      "Epoch 460/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4779 - acc: 0.7656 - val_loss: 0.4954 - val_acc: 0.7656\n",
      "Epoch 461/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4778 - acc: 0.7656 - val_loss: 0.4954 - val_acc: 0.7656\n",
      "Epoch 462/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4777 - acc: 0.7656 - val_loss: 0.4954 - val_acc: 0.7656\n",
      "Epoch 463/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4777 - acc: 0.7656 - val_loss: 0.4953 - val_acc: 0.7656\n",
      "Epoch 464/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4776 - acc: 0.7656 - val_loss: 0.4953 - val_acc: 0.7656\n",
      "Epoch 465/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4775 - acc: 0.7656 - val_loss: 0.4952 - val_acc: 0.7656\n",
      "Epoch 466/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4775 - acc: 0.7656 - val_loss: 0.4952 - val_acc: 0.7656\n",
      "Epoch 467/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4774 - acc: 0.7639 - val_loss: 0.4951 - val_acc: 0.7656\n",
      "Epoch 468/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4773 - acc: 0.7639 - val_loss: 0.4951 - val_acc: 0.7656\n",
      "Epoch 469/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4773 - acc: 0.7639 - val_loss: 0.4951 - val_acc: 0.7656\n",
      "Epoch 470/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4772 - acc: 0.7639 - val_loss: 0.4950 - val_acc: 0.7656\n",
      "Epoch 471/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4771 - acc: 0.7639 - val_loss: 0.4950 - val_acc: 0.7656\n",
      "Epoch 472/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4771 - acc: 0.7656 - val_loss: 0.4949 - val_acc: 0.7656\n",
      "Epoch 473/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4770 - acc: 0.7639 - val_loss: 0.4949 - val_acc: 0.7656\n",
      "Epoch 474/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4769 - acc: 0.7639 - val_loss: 0.4948 - val_acc: 0.7656\n",
      "Epoch 475/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4768 - acc: 0.7639 - val_loss: 0.4948 - val_acc: 0.7656\n",
      "Epoch 476/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4768 - acc: 0.7639 - val_loss: 0.4948 - val_acc: 0.7656\n",
      "Epoch 477/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4767 - acc: 0.7639 - val_loss: 0.4947 - val_acc: 0.7656\n",
      "Epoch 478/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4766 - acc: 0.7639 - val_loss: 0.4947 - val_acc: 0.7656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 479/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4766 - acc: 0.7639 - val_loss: 0.4946 - val_acc: 0.7656\n",
      "Epoch 480/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4765 - acc: 0.7639 - val_loss: 0.4946 - val_acc: 0.7656\n",
      "Epoch 481/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4765 - acc: 0.7639 - val_loss: 0.4946 - val_acc: 0.7656\n",
      "Epoch 482/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4764 - acc: 0.7639 - val_loss: 0.4945 - val_acc: 0.7656\n",
      "Epoch 483/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4763 - acc: 0.7639 - val_loss: 0.4945 - val_acc: 0.7656\n",
      "Epoch 484/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4763 - acc: 0.7639 - val_loss: 0.4944 - val_acc: 0.7656\n",
      "Epoch 485/1000\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4762 - acc: 0.7639 - val_loss: 0.4944 - val_acc: 0.7656\n",
      "Epoch 486/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4761 - acc: 0.7639 - val_loss: 0.4944 - val_acc: 0.7656\n",
      "Epoch 487/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4761 - acc: 0.7639 - val_loss: 0.4943 - val_acc: 0.7656\n",
      "Epoch 488/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4760 - acc: 0.7639 - val_loss: 0.4943 - val_acc: 0.7656\n",
      "Epoch 489/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4759 - acc: 0.7639 - val_loss: 0.4943 - val_acc: 0.7656\n",
      "Epoch 490/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4759 - acc: 0.7639 - val_loss: 0.4942 - val_acc: 0.7656\n",
      "Epoch 491/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4758 - acc: 0.7639 - val_loss: 0.4942 - val_acc: 0.7656\n",
      "Epoch 492/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4758 - acc: 0.7639 - val_loss: 0.4941 - val_acc: 0.7656\n",
      "Epoch 493/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4757 - acc: 0.7639 - val_loss: 0.4941 - val_acc: 0.7656\n",
      "Epoch 494/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4756 - acc: 0.7639 - val_loss: 0.4941 - val_acc: 0.7656\n",
      "Epoch 495/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4756 - acc: 0.7639 - val_loss: 0.4940 - val_acc: 0.7656\n",
      "Epoch 496/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4755 - acc: 0.7639 - val_loss: 0.4940 - val_acc: 0.7656\n",
      "Epoch 497/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4754 - acc: 0.7622 - val_loss: 0.4940 - val_acc: 0.7656\n",
      "Epoch 498/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4754 - acc: 0.7622 - val_loss: 0.4939 - val_acc: 0.7656\n",
      "Epoch 499/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4753 - acc: 0.7639 - val_loss: 0.4939 - val_acc: 0.7656\n",
      "Epoch 500/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4753 - acc: 0.7622 - val_loss: 0.4938 - val_acc: 0.7656\n",
      "Epoch 501/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4752 - acc: 0.7622 - val_loss: 0.4938 - val_acc: 0.7656\n",
      "Epoch 502/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4751 - acc: 0.7622 - val_loss: 0.4938 - val_acc: 0.7656\n",
      "Epoch 503/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4751 - acc: 0.7622 - val_loss: 0.4937 - val_acc: 0.7656\n",
      "Epoch 504/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4750 - acc: 0.7622 - val_loss: 0.4937 - val_acc: 0.7656\n",
      "Epoch 505/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4749 - acc: 0.7622 - val_loss: 0.4937 - val_acc: 0.7656\n",
      "Epoch 506/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4749 - acc: 0.7622 - val_loss: 0.4936 - val_acc: 0.7656\n",
      "Epoch 507/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4748 - acc: 0.7622 - val_loss: 0.4936 - val_acc: 0.7656\n",
      "Epoch 508/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4747 - acc: 0.7622 - val_loss: 0.4936 - val_acc: 0.7656\n",
      "Epoch 509/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4747 - acc: 0.7622 - val_loss: 0.4935 - val_acc: 0.7656\n",
      "Epoch 510/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4746 - acc: 0.7622 - val_loss: 0.4935 - val_acc: 0.7656\n",
      "Epoch 511/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4746 - acc: 0.7622 - val_loss: 0.4935 - val_acc: 0.7656\n",
      "Epoch 512/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4745 - acc: 0.7622 - val_loss: 0.4934 - val_acc: 0.7656\n",
      "Epoch 513/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4745 - acc: 0.7622 - val_loss: 0.4934 - val_acc: 0.7656\n",
      "Epoch 514/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4744 - acc: 0.7639 - val_loss: 0.4934 - val_acc: 0.7656\n",
      "Epoch 515/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4744 - acc: 0.7622 - val_loss: 0.4933 - val_acc: 0.7656\n",
      "Epoch 516/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4743 - acc: 0.7639 - val_loss: 0.4933 - val_acc: 0.7656\n",
      "Epoch 517/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4742 - acc: 0.7622 - val_loss: 0.4933 - val_acc: 0.7656\n",
      "Epoch 518/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4742 - acc: 0.7656 - val_loss: 0.4932 - val_acc: 0.7656\n",
      "Epoch 519/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4741 - acc: 0.7656 - val_loss: 0.4932 - val_acc: 0.7656\n",
      "Epoch 520/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4740 - acc: 0.7639 - val_loss: 0.4932 - val_acc: 0.7656\n",
      "Epoch 521/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4740 - acc: 0.7656 - val_loss: 0.4931 - val_acc: 0.7656\n",
      "Epoch 522/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4739 - acc: 0.7656 - val_loss: 0.4931 - val_acc: 0.7604\n",
      "Epoch 523/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4739 - acc: 0.7656 - val_loss: 0.4931 - val_acc: 0.7604\n",
      "Epoch 524/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4738 - acc: 0.7656 - val_loss: 0.4930 - val_acc: 0.7604\n",
      "Epoch 525/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4738 - acc: 0.7656 - val_loss: 0.4930 - val_acc: 0.7604\n",
      "Epoch 526/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4737 - acc: 0.7656 - val_loss: 0.4930 - val_acc: 0.7604\n",
      "Epoch 527/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4737 - acc: 0.7656 - val_loss: 0.4929 - val_acc: 0.7604\n",
      "Epoch 528/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4736 - acc: 0.7656 - val_loss: 0.4929 - val_acc: 0.7604\n",
      "Epoch 529/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4735 - acc: 0.7656 - val_loss: 0.4929 - val_acc: 0.7604\n",
      "Epoch 530/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4735 - acc: 0.7656 - val_loss: 0.4928 - val_acc: 0.7604\n",
      "Epoch 531/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4734 - acc: 0.7656 - val_loss: 0.4928 - val_acc: 0.7604\n",
      "Epoch 532/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4734 - acc: 0.7656 - val_loss: 0.4928 - val_acc: 0.7604\n",
      "Epoch 533/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4733 - acc: 0.7656 - val_loss: 0.4927 - val_acc: 0.7604\n",
      "Epoch 534/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4733 - acc: 0.7656 - val_loss: 0.4927 - val_acc: 0.7604\n",
      "Epoch 535/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4732 - acc: 0.7639 - val_loss: 0.4927 - val_acc: 0.7604\n",
      "Epoch 536/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4732 - acc: 0.7656 - val_loss: 0.4927 - val_acc: 0.7604\n",
      "Epoch 537/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4731 - acc: 0.7639 - val_loss: 0.4926 - val_acc: 0.7604\n",
      "Epoch 538/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4730 - acc: 0.7639 - val_loss: 0.4926 - val_acc: 0.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 539/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4730 - acc: 0.7656 - val_loss: 0.4926 - val_acc: 0.7604\n",
      "Epoch 540/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4729 - acc: 0.7639 - val_loss: 0.4925 - val_acc: 0.7604\n",
      "Epoch 541/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4729 - acc: 0.7639 - val_loss: 0.4925 - val_acc: 0.7604\n",
      "Epoch 542/1000\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4729 - acc: 0.7656 - val_loss: 0.4925 - val_acc: 0.7604\n",
      "Epoch 543/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4728 - acc: 0.7639 - val_loss: 0.4924 - val_acc: 0.7604\n",
      "Epoch 544/1000\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4727 - acc: 0.7639 - val_loss: 0.4924 - val_acc: 0.7604\n",
      "Epoch 545/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4727 - acc: 0.7639 - val_loss: 0.4924 - val_acc: 0.7604\n",
      "Epoch 546/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4726 - acc: 0.7656 - val_loss: 0.4924 - val_acc: 0.7604\n",
      "Epoch 547/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4726 - acc: 0.7639 - val_loss: 0.4923 - val_acc: 0.7604\n",
      "Epoch 548/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4725 - acc: 0.7639 - val_loss: 0.4923 - val_acc: 0.7604\n",
      "Epoch 549/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4725 - acc: 0.7639 - val_loss: 0.4923 - val_acc: 0.7604\n",
      "Epoch 550/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4724 - acc: 0.7639 - val_loss: 0.4922 - val_acc: 0.7604\n",
      "Epoch 551/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4724 - acc: 0.7639 - val_loss: 0.4922 - val_acc: 0.7604\n",
      "Epoch 552/1000\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4723 - acc: 0.7639 - val_loss: 0.4922 - val_acc: 0.7604\n",
      "Epoch 553/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4723 - acc: 0.7639 - val_loss: 0.4922 - val_acc: 0.7604\n",
      "Epoch 554/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4722 - acc: 0.7639 - val_loss: 0.4921 - val_acc: 0.7604\n",
      "Epoch 555/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4722 - acc: 0.7639 - val_loss: 0.4921 - val_acc: 0.7604\n",
      "Epoch 556/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4721 - acc: 0.7639 - val_loss: 0.4921 - val_acc: 0.7604\n",
      "Epoch 557/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4721 - acc: 0.7639 - val_loss: 0.4921 - val_acc: 0.7604\n",
      "Epoch 558/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4720 - acc: 0.7639 - val_loss: 0.4920 - val_acc: 0.7604\n",
      "Epoch 559/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4720 - acc: 0.7656 - val_loss: 0.4920 - val_acc: 0.7604\n",
      "Epoch 560/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4719 - acc: 0.7639 - val_loss: 0.4920 - val_acc: 0.7604\n",
      "Epoch 561/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4719 - acc: 0.7656 - val_loss: 0.4919 - val_acc: 0.7604\n",
      "Epoch 562/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4718 - acc: 0.7656 - val_loss: 0.4919 - val_acc: 0.7604\n",
      "Epoch 563/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4717 - acc: 0.7656 - val_loss: 0.4919 - val_acc: 0.7604\n",
      "Epoch 564/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4717 - acc: 0.7656 - val_loss: 0.4919 - val_acc: 0.7604\n",
      "Epoch 565/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4717 - acc: 0.7656 - val_loss: 0.4918 - val_acc: 0.7604\n",
      "Epoch 566/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4716 - acc: 0.7656 - val_loss: 0.4918 - val_acc: 0.7604\n",
      "Epoch 567/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4716 - acc: 0.7656 - val_loss: 0.4918 - val_acc: 0.7604\n",
      "Epoch 568/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4715 - acc: 0.7656 - val_loss: 0.4918 - val_acc: 0.7604\n",
      "Epoch 569/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4714 - acc: 0.7656 - val_loss: 0.4917 - val_acc: 0.7604\n",
      "Epoch 570/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4714 - acc: 0.7656 - val_loss: 0.4917 - val_acc: 0.7604\n",
      "Epoch 571/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4714 - acc: 0.7656 - val_loss: 0.4917 - val_acc: 0.7604\n",
      "Epoch 572/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4713 - acc: 0.7656 - val_loss: 0.4917 - val_acc: 0.7604\n",
      "Epoch 573/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4713 - acc: 0.7656 - val_loss: 0.4916 - val_acc: 0.7604\n",
      "Epoch 574/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4712 - acc: 0.7656 - val_loss: 0.4916 - val_acc: 0.7604\n",
      "Epoch 575/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4712 - acc: 0.7656 - val_loss: 0.4916 - val_acc: 0.7604\n",
      "Epoch 576/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4711 - acc: 0.7656 - val_loss: 0.4916 - val_acc: 0.7604\n",
      "Epoch 577/1000\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4711 - acc: 0.7656 - val_loss: 0.4915 - val_acc: 0.7604\n",
      "Epoch 578/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4710 - acc: 0.7656 - val_loss: 0.4915 - val_acc: 0.7604\n",
      "Epoch 579/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4710 - acc: 0.7656 - val_loss: 0.4915 - val_acc: 0.7604\n",
      "Epoch 580/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4709 - acc: 0.7656 - val_loss: 0.4915 - val_acc: 0.7552\n",
      "Epoch 581/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4709 - acc: 0.7656 - val_loss: 0.4914 - val_acc: 0.7552\n",
      "Epoch 582/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4708 - acc: 0.7656 - val_loss: 0.4914 - val_acc: 0.7552\n",
      "Epoch 583/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4708 - acc: 0.7656 - val_loss: 0.4914 - val_acc: 0.7552\n",
      "Epoch 584/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4708 - acc: 0.7656 - val_loss: 0.4914 - val_acc: 0.7552\n",
      "Epoch 585/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4707 - acc: 0.7656 - val_loss: 0.4913 - val_acc: 0.7552\n",
      "Epoch 586/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4706 - acc: 0.7656 - val_loss: 0.4913 - val_acc: 0.7552\n",
      "Epoch 587/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4706 - acc: 0.7674 - val_loss: 0.4913 - val_acc: 0.7552\n",
      "Epoch 588/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4706 - acc: 0.7656 - val_loss: 0.4913 - val_acc: 0.7552\n",
      "Epoch 589/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4705 - acc: 0.7674 - val_loss: 0.4913 - val_acc: 0.7552\n",
      "Epoch 590/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4705 - acc: 0.7656 - val_loss: 0.4912 - val_acc: 0.7552\n",
      "Epoch 591/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4704 - acc: 0.7656 - val_loss: 0.4912 - val_acc: 0.7552\n",
      "Epoch 592/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4704 - acc: 0.7656 - val_loss: 0.4912 - val_acc: 0.7552\n",
      "Epoch 593/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4703 - acc: 0.7674 - val_loss: 0.4912 - val_acc: 0.7552\n",
      "Epoch 594/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4703 - acc: 0.7674 - val_loss: 0.4911 - val_acc: 0.7552\n",
      "Epoch 595/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4702 - acc: 0.7674 - val_loss: 0.4911 - val_acc: 0.7552\n",
      "Epoch 596/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4702 - acc: 0.7674 - val_loss: 0.4911 - val_acc: 0.7552\n",
      "Epoch 597/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4702 - acc: 0.7674 - val_loss: 0.4911 - val_acc: 0.7552\n",
      "Epoch 598/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4701 - acc: 0.7674 - val_loss: 0.4910 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 599/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4701 - acc: 0.7674 - val_loss: 0.4910 - val_acc: 0.7552\n",
      "Epoch 600/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4700 - acc: 0.7674 - val_loss: 0.4910 - val_acc: 0.7552\n",
      "Epoch 601/1000\n",
      "576/576 [==============================] - 0s 115us/step - loss: 0.4700 - acc: 0.7674 - val_loss: 0.4910 - val_acc: 0.7552\n",
      "Epoch 602/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4699 - acc: 0.7674 - val_loss: 0.4910 - val_acc: 0.7552\n",
      "Epoch 603/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4699 - acc: 0.7674 - val_loss: 0.4909 - val_acc: 0.7552\n",
      "Epoch 604/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4698 - acc: 0.7656 - val_loss: 0.4909 - val_acc: 0.7552\n",
      "Epoch 605/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4698 - acc: 0.7656 - val_loss: 0.4909 - val_acc: 0.7552\n",
      "Epoch 606/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4698 - acc: 0.7656 - val_loss: 0.4909 - val_acc: 0.7552\n",
      "Epoch 607/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4697 - acc: 0.7656 - val_loss: 0.4909 - val_acc: 0.7552\n",
      "Epoch 608/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4697 - acc: 0.7656 - val_loss: 0.4908 - val_acc: 0.7552\n",
      "Epoch 609/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4696 - acc: 0.7656 - val_loss: 0.4908 - val_acc: 0.7552\n",
      "Epoch 610/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4696 - acc: 0.7656 - val_loss: 0.4908 - val_acc: 0.7552\n",
      "Epoch 611/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4695 - acc: 0.7656 - val_loss: 0.4908 - val_acc: 0.7552\n",
      "Epoch 612/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4695 - acc: 0.7656 - val_loss: 0.4907 - val_acc: 0.7552\n",
      "Epoch 613/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4695 - acc: 0.7656 - val_loss: 0.4907 - val_acc: 0.7552\n",
      "Epoch 614/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4694 - acc: 0.7656 - val_loss: 0.4907 - val_acc: 0.7552\n",
      "Epoch 615/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4694 - acc: 0.7656 - val_loss: 0.4907 - val_acc: 0.7552\n",
      "Epoch 616/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4693 - acc: 0.7656 - val_loss: 0.4907 - val_acc: 0.7552\n",
      "Epoch 617/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4693 - acc: 0.7656 - val_loss: 0.4906 - val_acc: 0.7552\n",
      "Epoch 618/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4693 - acc: 0.7656 - val_loss: 0.4906 - val_acc: 0.7552\n",
      "Epoch 619/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4692 - acc: 0.7656 - val_loss: 0.4906 - val_acc: 0.7552\n",
      "Epoch 620/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4692 - acc: 0.7656 - val_loss: 0.4906 - val_acc: 0.7552\n",
      "Epoch 621/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4691 - acc: 0.7656 - val_loss: 0.4906 - val_acc: 0.7552\n",
      "Epoch 622/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4691 - acc: 0.7656 - val_loss: 0.4906 - val_acc: 0.7552\n",
      "Epoch 623/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4690 - acc: 0.7656 - val_loss: 0.4905 - val_acc: 0.7552\n",
      "Epoch 624/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5988 - acc: 0.687 - 0s 63us/step - loss: 0.4690 - acc: 0.7656 - val_loss: 0.4905 - val_acc: 0.7552\n",
      "Epoch 625/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4690 - acc: 0.7656 - val_loss: 0.4905 - val_acc: 0.7552\n",
      "Epoch 626/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4689 - acc: 0.7656 - val_loss: 0.4905 - val_acc: 0.7552\n",
      "Epoch 627/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4689 - acc: 0.7656 - val_loss: 0.4905 - val_acc: 0.7552\n",
      "Epoch 628/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4688 - acc: 0.7656 - val_loss: 0.4904 - val_acc: 0.7552\n",
      "Epoch 629/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4688 - acc: 0.7656 - val_loss: 0.4904 - val_acc: 0.7552\n",
      "Epoch 630/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4688 - acc: 0.7656 - val_loss: 0.4904 - val_acc: 0.7552\n",
      "Epoch 631/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4687 - acc: 0.7656 - val_loss: 0.4904 - val_acc: 0.7552\n",
      "Epoch 632/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4687 - acc: 0.7674 - val_loss: 0.4904 - val_acc: 0.7552\n",
      "Epoch 633/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4687 - acc: 0.7691 - val_loss: 0.4903 - val_acc: 0.7552\n",
      "Epoch 634/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4686 - acc: 0.7691 - val_loss: 0.4903 - val_acc: 0.7552\n",
      "Epoch 635/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4686 - acc: 0.7691 - val_loss: 0.4903 - val_acc: 0.7552\n",
      "Epoch 636/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4685 - acc: 0.7691 - val_loss: 0.4903 - val_acc: 0.7552\n",
      "Epoch 637/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4685 - acc: 0.7691 - val_loss: 0.4903 - val_acc: 0.7552\n",
      "Epoch 638/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4685 - acc: 0.7691 - val_loss: 0.4903 - val_acc: 0.7552\n",
      "Epoch 639/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4684 - acc: 0.7691 - val_loss: 0.4902 - val_acc: 0.7552\n",
      "Epoch 640/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4684 - acc: 0.7691 - val_loss: 0.4902 - val_acc: 0.7552\n",
      "Epoch 641/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4683 - acc: 0.7691 - val_loss: 0.4902 - val_acc: 0.7500\n",
      "Epoch 642/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4683 - acc: 0.7691 - val_loss: 0.4902 - val_acc: 0.7500\n",
      "Epoch 643/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4683 - acc: 0.7691 - val_loss: 0.4902 - val_acc: 0.7500\n",
      "Epoch 644/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4682 - acc: 0.7691 - val_loss: 0.4901 - val_acc: 0.7500\n",
      "Epoch 645/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4682 - acc: 0.7691 - val_loss: 0.4901 - val_acc: 0.7500\n",
      "Epoch 646/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4681 - acc: 0.7691 - val_loss: 0.4901 - val_acc: 0.7500\n",
      "Epoch 647/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4681 - acc: 0.7691 - val_loss: 0.4901 - val_acc: 0.7500\n",
      "Epoch 648/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4681 - acc: 0.7708 - val_loss: 0.4901 - val_acc: 0.7500\n",
      "Epoch 649/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4680 - acc: 0.7691 - val_loss: 0.4901 - val_acc: 0.7500\n",
      "Epoch 650/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4680 - acc: 0.7708 - val_loss: 0.4900 - val_acc: 0.7500\n",
      "Epoch 651/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4679 - acc: 0.7708 - val_loss: 0.4900 - val_acc: 0.7500\n",
      "Epoch 652/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4679 - acc: 0.7708 - val_loss: 0.4900 - val_acc: 0.7500\n",
      "Epoch 653/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4679 - acc: 0.7708 - val_loss: 0.4900 - val_acc: 0.7500\n",
      "Epoch 654/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4678 - acc: 0.7708 - val_loss: 0.4900 - val_acc: 0.7500\n",
      "Epoch 655/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4678 - acc: 0.7708 - val_loss: 0.4900 - val_acc: 0.7500\n",
      "Epoch 656/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4678 - acc: 0.7708 - val_loss: 0.4899 - val_acc: 0.7500\n",
      "Epoch 657/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4677 - acc: 0.7708 - val_loss: 0.4899 - val_acc: 0.7500\n",
      "Epoch 658/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4677 - acc: 0.7708 - val_loss: 0.4899 - val_acc: 0.7500\n",
      "Epoch 659/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4677 - acc: 0.7708 - val_loss: 0.4899 - val_acc: 0.7500\n",
      "Epoch 660/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4676 - acc: 0.7708 - val_loss: 0.4899 - val_acc: 0.7500\n",
      "Epoch 661/1000\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4676 - acc: 0.7708 - val_loss: 0.4899 - val_acc: 0.7500\n",
      "Epoch 662/1000\n",
      "576/576 [==============================] - 0s 107us/step - loss: 0.4676 - acc: 0.7708 - val_loss: 0.4899 - val_acc: 0.7500\n",
      "Epoch 663/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4675 - acc: 0.7726 - val_loss: 0.4898 - val_acc: 0.7500\n",
      "Epoch 664/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4675 - acc: 0.7726 - val_loss: 0.4898 - val_acc: 0.7500\n",
      "Epoch 665/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4674 - acc: 0.7708 - val_loss: 0.4898 - val_acc: 0.7500\n",
      "Epoch 666/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4674 - acc: 0.7726 - val_loss: 0.4898 - val_acc: 0.7500\n",
      "Epoch 667/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4674 - acc: 0.7726 - val_loss: 0.4898 - val_acc: 0.7500\n",
      "Epoch 668/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4673 - acc: 0.7726 - val_loss: 0.4898 - val_acc: 0.7500\n",
      "Epoch 669/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4673 - acc: 0.7708 - val_loss: 0.4897 - val_acc: 0.7500\n",
      "Epoch 670/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4673 - acc: 0.7726 - val_loss: 0.4897 - val_acc: 0.7500\n",
      "Epoch 671/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4672 - acc: 0.7726 - val_loss: 0.4897 - val_acc: 0.7500\n",
      "Epoch 672/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4672 - acc: 0.7726 - val_loss: 0.4897 - val_acc: 0.7500\n",
      "Epoch 673/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4672 - acc: 0.7726 - val_loss: 0.4897 - val_acc: 0.7500\n",
      "Epoch 674/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4671 - acc: 0.7708 - val_loss: 0.4897 - val_acc: 0.7500\n",
      "Epoch 675/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4671 - acc: 0.7726 - val_loss: 0.4897 - val_acc: 0.7500\n",
      "Epoch 676/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4671 - acc: 0.7726 - val_loss: 0.4896 - val_acc: 0.7500\n",
      "Epoch 677/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4670 - acc: 0.7726 - val_loss: 0.4896 - val_acc: 0.7500\n",
      "Epoch 678/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4670 - acc: 0.7726 - val_loss: 0.4896 - val_acc: 0.7500\n",
      "Epoch 679/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4670 - acc: 0.7726 - val_loss: 0.4896 - val_acc: 0.7500\n",
      "Epoch 680/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4669 - acc: 0.7708 - val_loss: 0.4896 - val_acc: 0.7500\n",
      "Epoch 681/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4669 - acc: 0.7708 - val_loss: 0.4896 - val_acc: 0.7500\n",
      "Epoch 682/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4668 - acc: 0.7708 - val_loss: 0.4896 - val_acc: 0.7500\n",
      "Epoch 683/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4668 - acc: 0.7708 - val_loss: 0.4895 - val_acc: 0.7500\n",
      "Epoch 684/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4668 - acc: 0.7708 - val_loss: 0.4895 - val_acc: 0.7500\n",
      "Epoch 685/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4668 - acc: 0.7708 - val_loss: 0.4895 - val_acc: 0.7500\n",
      "Epoch 686/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4667 - acc: 0.7708 - val_loss: 0.4895 - val_acc: 0.7500\n",
      "Epoch 687/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4667 - acc: 0.7708 - val_loss: 0.4895 - val_acc: 0.7500\n",
      "Epoch 688/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4667 - acc: 0.7708 - val_loss: 0.4895 - val_acc: 0.7500\n",
      "Epoch 689/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4666 - acc: 0.7708 - val_loss: 0.4895 - val_acc: 0.7500\n",
      "Epoch 690/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4666 - acc: 0.7708 - val_loss: 0.4894 - val_acc: 0.7500\n",
      "Epoch 691/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4666 - acc: 0.7708 - val_loss: 0.4894 - val_acc: 0.7500\n",
      "Epoch 692/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4665 - acc: 0.7708 - val_loss: 0.4894 - val_acc: 0.7500\n",
      "Epoch 693/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4665 - acc: 0.7708 - val_loss: 0.4894 - val_acc: 0.7500\n",
      "Epoch 694/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4665 - acc: 0.7708 - val_loss: 0.4894 - val_acc: 0.7500\n",
      "Epoch 695/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4664 - acc: 0.7708 - val_loss: 0.4894 - val_acc: 0.7500\n",
      "Epoch 696/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4664 - acc: 0.7708 - val_loss: 0.4894 - val_acc: 0.7500\n",
      "Epoch 697/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4664 - acc: 0.7708 - val_loss: 0.4893 - val_acc: 0.7500\n",
      "Epoch 698/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4663 - acc: 0.7708 - val_loss: 0.4893 - val_acc: 0.7500\n",
      "Epoch 699/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4663 - acc: 0.7708 - val_loss: 0.4893 - val_acc: 0.7500\n",
      "Epoch 700/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4663 - acc: 0.7708 - val_loss: 0.4893 - val_acc: 0.7500\n",
      "Epoch 701/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4662 - acc: 0.7708 - val_loss: 0.4893 - val_acc: 0.7500\n",
      "Epoch 702/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4662 - acc: 0.7708 - val_loss: 0.4893 - val_acc: 0.7500\n",
      "Epoch 703/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4662 - acc: 0.7708 - val_loss: 0.4893 - val_acc: 0.7500\n",
      "Epoch 704/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4661 - acc: 0.7708 - val_loss: 0.4893 - val_acc: 0.7500\n",
      "Epoch 705/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4661 - acc: 0.7708 - val_loss: 0.4892 - val_acc: 0.7500\n",
      "Epoch 706/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4661 - acc: 0.7708 - val_loss: 0.4892 - val_acc: 0.7500\n",
      "Epoch 707/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4660 - acc: 0.7708 - val_loss: 0.4892 - val_acc: 0.7500\n",
      "Epoch 708/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5954 - acc: 0.625 - 0s 57us/step - loss: 0.4660 - acc: 0.7708 - val_loss: 0.4892 - val_acc: 0.7500\n",
      "Epoch 709/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4660 - acc: 0.7708 - val_loss: 0.4892 - val_acc: 0.7500\n",
      "Epoch 710/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4660 - acc: 0.7708 - val_loss: 0.4892 - val_acc: 0.7500\n",
      "Epoch 711/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4659 - acc: 0.7708 - val_loss: 0.4892 - val_acc: 0.7500\n",
      "Epoch 712/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4659 - acc: 0.7726 - val_loss: 0.4892 - val_acc: 0.7500\n",
      "Epoch 713/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4659 - acc: 0.7726 - val_loss: 0.4892 - val_acc: 0.7500\n",
      "Epoch 714/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4658 - acc: 0.7726 - val_loss: 0.4891 - val_acc: 0.7500\n",
      "Epoch 715/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4658 - acc: 0.7726 - val_loss: 0.4891 - val_acc: 0.7500\n",
      "Epoch 716/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4658 - acc: 0.7726 - val_loss: 0.4891 - val_acc: 0.7500\n",
      "Epoch 717/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 63us/step - loss: 0.4657 - acc: 0.7726 - val_loss: 0.4891 - val_acc: 0.7500\n",
      "Epoch 718/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4657 - acc: 0.7726 - val_loss: 0.4891 - val_acc: 0.7500\n",
      "Epoch 719/1000\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.4657 - acc: 0.7726 - val_loss: 0.4891 - val_acc: 0.7500\n",
      "Epoch 720/1000\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4656 - acc: 0.7743 - val_loss: 0.4891 - val_acc: 0.7500\n",
      "Epoch 721/1000\n",
      "576/576 [==============================] - 0s 115us/step - loss: 0.4656 - acc: 0.7726 - val_loss: 0.4891 - val_acc: 0.7500\n",
      "Epoch 722/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4656 - acc: 0.7743 - val_loss: 0.4891 - val_acc: 0.7500\n",
      "Epoch 723/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4656 - acc: 0.7743 - val_loss: 0.4890 - val_acc: 0.7500\n",
      "Epoch 724/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4655 - acc: 0.7726 - val_loss: 0.4890 - val_acc: 0.7500\n",
      "Epoch 725/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4655 - acc: 0.7743 - val_loss: 0.4890 - val_acc: 0.7500\n",
      "Epoch 726/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4655 - acc: 0.7743 - val_loss: 0.4890 - val_acc: 0.7500\n",
      "Epoch 727/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4654 - acc: 0.7743 - val_loss: 0.4890 - val_acc: 0.7500\n",
      "Epoch 728/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4654 - acc: 0.7743 - val_loss: 0.4890 - val_acc: 0.7500\n",
      "Epoch 729/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4654 - acc: 0.7760 - val_loss: 0.4890 - val_acc: 0.7500\n",
      "Epoch 730/1000\n",
      "576/576 [==============================] - 0s 49us/step - loss: 0.4654 - acc: 0.7743 - val_loss: 0.4890 - val_acc: 0.7500\n",
      "Epoch 731/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4653 - acc: 0.7743 - val_loss: 0.4890 - val_acc: 0.7500\n",
      "Epoch 732/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4653 - acc: 0.7760 - val_loss: 0.4889 - val_acc: 0.7500\n",
      "Epoch 733/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4653 - acc: 0.7743 - val_loss: 0.4889 - val_acc: 0.7500\n",
      "Epoch 734/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4652 - acc: 0.7743 - val_loss: 0.4889 - val_acc: 0.7500\n",
      "Epoch 735/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4652 - acc: 0.7760 - val_loss: 0.4889 - val_acc: 0.7500\n",
      "Epoch 736/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4652 - acc: 0.7760 - val_loss: 0.4889 - val_acc: 0.7500\n",
      "Epoch 737/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4651 - acc: 0.7760 - val_loss: 0.4889 - val_acc: 0.7500\n",
      "Epoch 738/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4651 - acc: 0.7760 - val_loss: 0.4889 - val_acc: 0.7500\n",
      "Epoch 739/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4651 - acc: 0.7760 - val_loss: 0.4889 - val_acc: 0.7500\n",
      "Epoch 740/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4651 - acc: 0.7760 - val_loss: 0.4889 - val_acc: 0.7500\n",
      "Epoch 741/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4650 - acc: 0.7760 - val_loss: 0.4889 - val_acc: 0.7500\n",
      "Epoch 742/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4650 - acc: 0.7760 - val_loss: 0.4888 - val_acc: 0.7500\n",
      "Epoch 743/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4650 - acc: 0.7760 - val_loss: 0.4888 - val_acc: 0.7500\n",
      "Epoch 744/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4650 - acc: 0.7743 - val_loss: 0.4888 - val_acc: 0.7500\n",
      "Epoch 745/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4649 - acc: 0.7760 - val_loss: 0.4888 - val_acc: 0.7500\n",
      "Epoch 746/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4649 - acc: 0.7760 - val_loss: 0.4888 - val_acc: 0.7500\n",
      "Epoch 747/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4649 - acc: 0.7743 - val_loss: 0.4888 - val_acc: 0.7500\n",
      "Epoch 748/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4649 - acc: 0.7743 - val_loss: 0.4888 - val_acc: 0.7500\n",
      "Epoch 749/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4648 - acc: 0.7760 - val_loss: 0.4888 - val_acc: 0.7552\n",
      "Epoch 750/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4648 - acc: 0.7760 - val_loss: 0.4888 - val_acc: 0.7552\n",
      "Epoch 751/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4648 - acc: 0.7760 - val_loss: 0.4888 - val_acc: 0.7552\n",
      "Epoch 752/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4647 - acc: 0.7760 - val_loss: 0.4887 - val_acc: 0.7552\n",
      "Epoch 753/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4647 - acc: 0.7760 - val_loss: 0.4887 - val_acc: 0.7552\n",
      "Epoch 754/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4647 - acc: 0.7760 - val_loss: 0.4887 - val_acc: 0.7552\n",
      "Epoch 755/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4647 - acc: 0.7760 - val_loss: 0.4887 - val_acc: 0.7552\n",
      "Epoch 756/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4647 - acc: 0.7760 - val_loss: 0.4887 - val_acc: 0.7552\n",
      "Epoch 757/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4646 - acc: 0.7760 - val_loss: 0.4887 - val_acc: 0.7552\n",
      "Epoch 758/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4646 - acc: 0.7760 - val_loss: 0.4887 - val_acc: 0.7552\n",
      "Epoch 759/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4646 - acc: 0.7760 - val_loss: 0.4887 - val_acc: 0.7552\n",
      "Epoch 760/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4645 - acc: 0.7760 - val_loss: 0.4887 - val_acc: 0.7552\n",
      "Epoch 761/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4645 - acc: 0.7760 - val_loss: 0.4887 - val_acc: 0.7552\n",
      "Epoch 762/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4645 - acc: 0.7760 - val_loss: 0.4887 - val_acc: 0.7552\n",
      "Epoch 763/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4645 - acc: 0.7760 - val_loss: 0.4886 - val_acc: 0.7552\n",
      "Epoch 764/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4644 - acc: 0.7760 - val_loss: 0.4886 - val_acc: 0.7552\n",
      "Epoch 765/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4644 - acc: 0.7760 - val_loss: 0.4886 - val_acc: 0.7552\n",
      "Epoch 766/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4644 - acc: 0.7760 - val_loss: 0.4886 - val_acc: 0.7552\n",
      "Epoch 767/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4644 - acc: 0.7760 - val_loss: 0.4886 - val_acc: 0.7552\n",
      "Epoch 768/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4643 - acc: 0.7760 - val_loss: 0.4886 - val_acc: 0.7552\n",
      "Epoch 769/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4643 - acc: 0.7760 - val_loss: 0.4886 - val_acc: 0.7552\n",
      "Epoch 770/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4643 - acc: 0.7760 - val_loss: 0.4886 - val_acc: 0.7552\n",
      "Epoch 771/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4642 - acc: 0.7760 - val_loss: 0.4886 - val_acc: 0.7552\n",
      "Epoch 772/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4642 - acc: 0.7760 - val_loss: 0.4886 - val_acc: 0.7552\n",
      "Epoch 773/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4642 - acc: 0.7760 - val_loss: 0.4886 - val_acc: 0.7552\n",
      "Epoch 774/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4642 - acc: 0.7760 - val_loss: 0.4886 - val_acc: 0.7552\n",
      "Epoch 775/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4642 - acc: 0.7760 - val_loss: 0.4885 - val_acc: 0.7552\n",
      "Epoch 776/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4641 - acc: 0.7760 - val_loss: 0.4885 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 777/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4641 - acc: 0.7760 - val_loss: 0.4885 - val_acc: 0.7552\n",
      "Epoch 778/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4641 - acc: 0.7760 - val_loss: 0.4885 - val_acc: 0.7552\n",
      "Epoch 779/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4640 - acc: 0.7760 - val_loss: 0.4885 - val_acc: 0.7552\n",
      "Epoch 780/1000\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4640 - acc: 0.7760 - val_loss: 0.4885 - val_acc: 0.7552\n",
      "Epoch 781/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4640 - acc: 0.7760 - val_loss: 0.4885 - val_acc: 0.7552\n",
      "Epoch 782/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4640 - acc: 0.7760 - val_loss: 0.4885 - val_acc: 0.7552\n",
      "Epoch 783/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4639 - acc: 0.7778 - val_loss: 0.4885 - val_acc: 0.7552\n",
      "Epoch 784/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4639 - acc: 0.7760 - val_loss: 0.4885 - val_acc: 0.7552\n",
      "Epoch 785/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4639 - acc: 0.7760 - val_loss: 0.4885 - val_acc: 0.7552\n",
      "Epoch 786/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4639 - acc: 0.7778 - val_loss: 0.4885 - val_acc: 0.7552\n",
      "Epoch 787/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4638 - acc: 0.7778 - val_loss: 0.4884 - val_acc: 0.7552\n",
      "Epoch 788/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4638 - acc: 0.7760 - val_loss: 0.4884 - val_acc: 0.7552\n",
      "Epoch 789/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4638 - acc: 0.7760 - val_loss: 0.4884 - val_acc: 0.7552\n",
      "Epoch 790/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4638 - acc: 0.7760 - val_loss: 0.4884 - val_acc: 0.7552\n",
      "Epoch 791/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4638 - acc: 0.7760 - val_loss: 0.4884 - val_acc: 0.7552\n",
      "Epoch 792/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4637 - acc: 0.7760 - val_loss: 0.4884 - val_acc: 0.7552\n",
      "Epoch 793/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4637 - acc: 0.7760 - val_loss: 0.4884 - val_acc: 0.7552\n",
      "Epoch 794/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4637 - acc: 0.7760 - val_loss: 0.4884 - val_acc: 0.7552\n",
      "Epoch 795/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4636 - acc: 0.7760 - val_loss: 0.4884 - val_acc: 0.7552\n",
      "Epoch 796/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4636 - acc: 0.7760 - val_loss: 0.4884 - val_acc: 0.7552\n",
      "Epoch 797/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4636 - acc: 0.7760 - val_loss: 0.4884 - val_acc: 0.7552\n",
      "Epoch 798/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4636 - acc: 0.7760 - val_loss: 0.4884 - val_acc: 0.7552\n",
      "Epoch 799/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4636 - acc: 0.7760 - val_loss: 0.4884 - val_acc: 0.7552\n",
      "Epoch 800/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4635 - acc: 0.7760 - val_loss: 0.4884 - val_acc: 0.7552\n",
      "Epoch 801/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4635 - acc: 0.7760 - val_loss: 0.4883 - val_acc: 0.7552\n",
      "Epoch 802/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4635 - acc: 0.7760 - val_loss: 0.4883 - val_acc: 0.7552\n",
      "Epoch 803/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4635 - acc: 0.7778 - val_loss: 0.4883 - val_acc: 0.7552\n",
      "Epoch 804/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4634 - acc: 0.7760 - val_loss: 0.4883 - val_acc: 0.7552\n",
      "Epoch 805/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4634 - acc: 0.7778 - val_loss: 0.4883 - val_acc: 0.7552\n",
      "Epoch 806/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4634 - acc: 0.7778 - val_loss: 0.4883 - val_acc: 0.7552\n",
      "Epoch 807/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4634 - acc: 0.7778 - val_loss: 0.4883 - val_acc: 0.7552\n",
      "Epoch 808/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4634 - acc: 0.7778 - val_loss: 0.4883 - val_acc: 0.7552\n",
      "Epoch 809/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4633 - acc: 0.7778 - val_loss: 0.4883 - val_acc: 0.7552\n",
      "Epoch 810/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4633 - acc: 0.7778 - val_loss: 0.4883 - val_acc: 0.7552\n",
      "Epoch 811/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4633 - acc: 0.7778 - val_loss: 0.4883 - val_acc: 0.7552\n",
      "Epoch 812/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4633 - acc: 0.7778 - val_loss: 0.4883 - val_acc: 0.7552\n",
      "Epoch 813/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4632 - acc: 0.7778 - val_loss: 0.4883 - val_acc: 0.7552\n",
      "Epoch 814/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4632 - acc: 0.7778 - val_loss: 0.4883 - val_acc: 0.7552\n",
      "Epoch 815/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4632 - acc: 0.7778 - val_loss: 0.4883 - val_acc: 0.7552\n",
      "Epoch 816/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4632 - acc: 0.7778 - val_loss: 0.4882 - val_acc: 0.7552\n",
      "Epoch 817/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4632 - acc: 0.7778 - val_loss: 0.4882 - val_acc: 0.7552\n",
      "Epoch 818/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4631 - acc: 0.7778 - val_loss: 0.4882 - val_acc: 0.7552\n",
      "Epoch 819/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4631 - acc: 0.7778 - val_loss: 0.4882 - val_acc: 0.7552\n",
      "Epoch 820/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4631 - acc: 0.7778 - val_loss: 0.4882 - val_acc: 0.7552\n",
      "Epoch 821/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4631 - acc: 0.7778 - val_loss: 0.4882 - val_acc: 0.7552\n",
      "Epoch 822/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4631 - acc: 0.7778 - val_loss: 0.4882 - val_acc: 0.7552\n",
      "Epoch 823/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4630 - acc: 0.7778 - val_loss: 0.4882 - val_acc: 0.7552\n",
      "Epoch 824/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4630 - acc: 0.7778 - val_loss: 0.4882 - val_acc: 0.7552\n",
      "Epoch 825/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4630 - acc: 0.7778 - val_loss: 0.4882 - val_acc: 0.7552\n",
      "Epoch 826/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4630 - acc: 0.7778 - val_loss: 0.4882 - val_acc: 0.7552\n",
      "Epoch 827/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4629 - acc: 0.7778 - val_loss: 0.4882 - val_acc: 0.7552\n",
      "Epoch 828/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4629 - acc: 0.7778 - val_loss: 0.4882 - val_acc: 0.7552\n",
      "Epoch 829/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4629 - acc: 0.7778 - val_loss: 0.4882 - val_acc: 0.7552\n",
      "Epoch 830/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4629 - acc: 0.7778 - val_loss: 0.4882 - val_acc: 0.7552\n",
      "Epoch 831/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4629 - acc: 0.7778 - val_loss: 0.4882 - val_acc: 0.7552\n",
      "Epoch 832/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4628 - acc: 0.7778 - val_loss: 0.4882 - val_acc: 0.7552\n",
      "Epoch 833/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4628 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 834/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4628 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 835/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4628 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 836/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4628 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 837/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4627 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 838/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4627 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 839/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4627 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 840/1000\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.4627 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 841/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4626 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 842/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4626 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 843/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4626 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 844/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4626 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 845/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4626 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 846/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4625 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 847/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4625 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 848/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4625 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 849/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4625 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 850/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4625 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 851/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4624 - acc: 0.7778 - val_loss: 0.4881 - val_acc: 0.7552\n",
      "Epoch 852/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4624 - acc: 0.7778 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 853/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4624 - acc: 0.7778 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 854/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4624 - acc: 0.7778 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 855/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4624 - acc: 0.7778 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 856/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4623 - acc: 0.7778 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 857/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4623 - acc: 0.7778 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 858/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4623 - acc: 0.7760 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 859/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4623 - acc: 0.7760 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 860/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4622 - acc: 0.7760 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 861/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4622 - acc: 0.7743 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 862/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4622 - acc: 0.7760 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 863/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4622 - acc: 0.7743 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 864/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4622 - acc: 0.7743 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 865/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4622 - acc: 0.7743 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 866/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4621 - acc: 0.7760 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 867/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4621 - acc: 0.7760 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 868/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4621 - acc: 0.7743 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 869/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4621 - acc: 0.7760 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 870/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4621 - acc: 0.7760 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 871/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4620 - acc: 0.7743 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 872/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4620 - acc: 0.7743 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 873/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4620 - acc: 0.7743 - val_loss: 0.4880 - val_acc: 0.7552\n",
      "Epoch 874/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4620 - acc: 0.7760 - val_loss: 0.4879 - val_acc: 0.7552\n",
      "Epoch 875/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4620 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7552\n",
      "Epoch 876/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4619 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7552\n",
      "Epoch 877/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4619 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 878/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4619 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 879/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4619 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 880/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4619 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 881/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4619 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 882/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4618 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 883/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4619 - acc: 0.7760 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 884/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.3216 - acc: 0.906 - 0s 68us/step - loss: 0.4618 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 885/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4618 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 886/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4618 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 887/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4617 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 888/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4617 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 889/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4617 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 890/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4617 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 891/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4617 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 892/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4617 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 893/1000\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.4616 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 894/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4616 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 895/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4616 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 896/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4616 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 897/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4616 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 898/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4616 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 899/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4615 - acc: 0.7743 - val_loss: 0.4879 - val_acc: 0.7500\n",
      "Epoch 900/1000\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.4615 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 901/1000\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4615 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 902/1000\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4615 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 903/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.5201 - acc: 0.687 - 0s 73us/step - loss: 0.4615 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 904/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4615 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 905/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4614 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 906/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4614 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 907/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4614 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 908/1000\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4614 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 909/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4614 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 910/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4613 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 911/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4613 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 912/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4613 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 913/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4613 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 914/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4613 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 915/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4612 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 916/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4613 - acc: 0.7726 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 917/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4612 - acc: 0.7726 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 918/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4612 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 919/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4612 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 920/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4612 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 921/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4612 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 922/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4611 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 923/1000\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.4611 - acc: 0.7726 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 924/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4611 - acc: 0.7726 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 925/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4611 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 926/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4611 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 927/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4611 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 928/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4610 - acc: 0.7726 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 929/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4610 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 930/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4610 - acc: 0.7726 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 931/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4610 - acc: 0.7743 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 932/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4610 - acc: 0.7726 - val_loss: 0.4878 - val_acc: 0.7500\n",
      "Epoch 933/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4609 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 934/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4610 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 935/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4609 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 936/1000\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.4609 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 937/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4609 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 938/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4609 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 939/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4609 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 940/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4609 - acc: 0.7743 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 941/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4608 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 942/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4608 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 943/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4608 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 944/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.6504 - acc: 0.625 - 0s 63us/step - loss: 0.4608 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 945/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4608 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 946/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4608 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 947/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4607 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 948/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4607 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 949/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4607 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 950/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4868 - acc: 0.750 - 0s 49us/step - loss: 0.4607 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 951/1000\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.4607 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 952/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4607 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 953/1000\n",
      "576/576 [==============================] - 0s 70us/step - loss: 0.4606 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 954/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4606 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 955/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 68us/step - loss: 0.4606 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 956/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4606 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 957/1000\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4606 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 958/1000\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4606 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 959/1000\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4606 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 960/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4605 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 961/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4605 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 962/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4605 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 963/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4605 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 964/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4605 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 965/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4605 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 966/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4605 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 967/1000\n",
      "576/576 [==============================] - 0s 57us/step - loss: 0.4604 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 968/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4604 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 969/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4604 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 970/1000\n",
      "576/576 [==============================] - 0s 59us/step - loss: 0.4604 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 971/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4604 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 972/1000\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4604 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 973/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4603 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 974/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4603 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 975/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4603 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 976/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4603 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 977/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4603 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 978/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4603 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 979/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4603 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 980/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4602 - acc: 0.7726 - val_loss: 0.4877 - val_acc: 0.7500\n",
      "Epoch 981/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4602 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7500\n",
      "Epoch 982/1000\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.4602 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7500\n",
      "Epoch 983/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4602 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7500\n",
      "Epoch 984/1000\n",
      "576/576 [==============================] - 0s 64us/step - loss: 0.4602 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7500\n",
      "Epoch 985/1000\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4602 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7500\n",
      "Epoch 986/1000\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.4602 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7500\n",
      "Epoch 987/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4602 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7500\n",
      "Epoch 988/1000\n",
      "576/576 [==============================] - 0s 61us/step - loss: 0.4601 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7500\n",
      "Epoch 989/1000\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4601 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7500\n",
      "Epoch 990/1000\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4601 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7500\n",
      "Epoch 991/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4601 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7500\n",
      "Epoch 992/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4601 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7500\n",
      "Epoch 993/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4601 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7500\n",
      "Epoch 994/1000\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.6177 - acc: 0.625 - 0s 52us/step - loss: 0.4601 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7500\n",
      "Epoch 995/1000\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.4600 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7552\n",
      "Epoch 996/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4600 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7552\n",
      "Epoch 997/1000\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.4600 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7552\n",
      "Epoch 998/1000\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.4600 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7552\n",
      "Epoch 999/1000\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4600 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7552\n",
      "Epoch 1000/1000\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.4600 - acc: 0.7726 - val_loss: 0.4876 - val_acc: 0.7552\n"
     ]
    }
   ],
   "source": [
    "## Note that when we call \"fit\" again, it picks up where it left off\n",
    "run_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x21805874be0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6sAAAHVCAYAAAAXVW0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21805175898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = len(run_hist_1.history[\"loss\"])\n",
    "m = len(run_hist_1b.history['loss'])\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, enquanto o erro de treino continua caindo, o de validação parece ter começado a piorar. Isso sugere que não devemos continuar com o treino. Qual o número apropriado de iterações?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício\n",
    "\n",
    "Repita os experimentos anteriores, sem ajuda:\n",
    "- Construa um modelo com duas camadas escondidas, cada uma com 6 nós\n",
    "- Use a ativação \"relu\" nas camadas escondidas e \"sigmoid\" para a camada final\n",
    "- Com uma taxa de aprendizado de .003 treine por 1500 iterações\n",
    "- Plote o erro e a acurácia na base de treino e teste ao longo das iterações\n",
    "- Plote a curva ROC-AUC para as predições\n",
    "\n",
    "Teste diferentes taxas de aprendizado, número de iterações e estruturas de rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-28cc7b87975b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_2 = Sequential([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential([\n",
    "    Dense(6, input_shape=(8,), activation='relu'),\n",
    "    Dense(6, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_2.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_2 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cant complete the test due to unavailability of the databse\n",
    "#all missing code though can be found above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
